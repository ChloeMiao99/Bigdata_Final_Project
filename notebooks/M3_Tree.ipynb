{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QAssr91yxZTI",
        "outputId": "b8e21592-4ba5-44bb-8c46-509a4e1fc07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ycuwtfqfxZTK",
        "outputId": "df24a910-1870-4e0f-90f5-d67796dee2b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/zongaobian/h1b-lca-disclosure-data-2020-2024?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.27G/1.27G [00:45<00:00, 29.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/zongaobian/h1b-lca-disclosure-data-2020-2024/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"zongaobian/h1b-lca-disclosure-data-2020-2024\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7M37KBAQxZTL",
        "outputId": "5e69f440-72e5-49f4-eaed-c03186d506c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the dataset directory: ['Combined_LCA_Disclosure_Data_FY2024.csv', 'Combined_LCA_Disclosure_Data_FY2023.csv', 'Combined_LCA_Disclosure_Data_FY2022.csv', 'Combined_LCA_Disclosure_Data_FY2021.csv', 'Combined_LCA_Disclosure_Data_FY2020_to_FY2024.csv', 'Combined_LCA_Disclosure_Data_FY2020.csv']\n"
          ]
        }
      ],
      "source": [
        "# List the files in the downloaded directory to see what files are available\n",
        "import os\n",
        "import pandas as pd\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset directory:\", files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yxhzAF4MxZTL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if 'spark' in locals():\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wPXqK6X6GWCT"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrame\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbVut3PpxZTM"
      },
      "source": [
        "# Join the tables\n",
        "References: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html\n",
        "\n",
        "Before doing EDA, we need to join the data of each year in 2020-2024 using union join."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AvYgTuFAxZTN",
        "outputId": "7f525cbb-e0a1-4c9a-cbe9-9b90eef5f31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joining: file:///root/.cache/kagglehub/datasets/zongaobian/h1b-lca-disclosure-data-2020-2024/versions/1/Combined_LCA_Disclosure_Data_FY2024.csv\n",
            "Joining: file:///root/.cache/kagglehub/datasets/zongaobian/h1b-lca-disclosure-data-2020-2024/versions/1/Combined_LCA_Disclosure_Data_FY2023.csv\n",
            "Joining: file:///root/.cache/kagglehub/datasets/zongaobian/h1b-lca-disclosure-data-2020-2024/versions/1/Combined_LCA_Disclosure_Data_FY2022.csv\n",
            "Joining: file:///root/.cache/kagglehub/datasets/zongaobian/h1b-lca-disclosure-data-2020-2024/versions/1/Combined_LCA_Disclosure_Data_FY2021.csv\n",
            "Joining: file:///root/.cache/kagglehub/datasets/zongaobian/h1b-lca-disclosure-data-2020-2024/versions/1/Combined_LCA_Disclosure_Data_FY2020.csv\n",
            "+------------------+-----------+-------------+-------------+------------------+--------------+--------------------+----------+--------------------+------------------+----------+----------+----------------------+--------------+--------------------+--------------------------+-------------------------+---------------+----------------+--------------------+--------------+--------------------+-----------------+-------------+--------------+--------------------+--------------------+-----------------+--------------+------------------+----------+----------------------+-----------------------+------------------------+----------------------+---------------------+---------------------+-----------------+------------------+------------------------+--------------------+---------------------+------------------+----------------------+--------------------+---------------------------+------------------------+-------------------------+--------------------------+-----------------------+-----------------------+-------------------+--------------------+--------------------------+----------------------+-----------------------+--------------------+------------------------+----------------------------+--------------------------+----------------------+---------------------------+----------------+----------------+------------------------------+--------------------+------------------+--------------+---------------+--------------+--------------------+---------------------+-------------------+----------------+---------------+--------------+------------------+-------------+--------------------+---------------+-------------+--------------------+--------------------+------------------------+---------------------+--------------+----------------+-----------+--------------------+-------------------+-----------------+------------------+-------------------+-----------------------+----------------------+--------------------+\n",
            "|       CASE_NUMBER|CASE_STATUS|RECEIVED_DATE|DECISION_DATE|ORIGINAL_CERT_DATE|    VISA_CLASS|           JOB_TITLE|  SOC_CODE|           SOC_TITLE|FULL_TIME_POSITION|BEGIN_DATE|  END_DATE|TOTAL_WORKER_POSITIONS|NEW_EMPLOYMENT|CONTINUED_EMPLOYMENT|CHANGE_PREVIOUS_EMPLOYMENT|NEW_CONCURRENT_EMPLOYMENT|CHANGE_EMPLOYER|AMENDED_PETITION|       EMPLOYER_NAME|TRADE_NAME_DBA|   EMPLOYER_ADDRESS1|EMPLOYER_ADDRESS2|EMPLOYER_CITY|EMPLOYER_STATE|EMPLOYER_POSTAL_CODE|    EMPLOYER_COUNTRY|EMPLOYER_PROVINCE|EMPLOYER_PHONE|EMPLOYER_PHONE_EXT|NAICS_CODE|EMPLOYER_POC_LAST_NAME|EMPLOYER_POC_FIRST_NAME|EMPLOYER_POC_MIDDLE_NAME|EMPLOYER_POC_JOB_TITLE|EMPLOYER_POC_ADDRESS1|EMPLOYER_POC_ADDRESS2|EMPLOYER_POC_CITY|EMPLOYER_POC_STATE|EMPLOYER_POC_POSTAL_CODE|EMPLOYER_POC_COUNTRY|EMPLOYER_POC_PROVINCE|EMPLOYER_POC_PHONE|EMPLOYER_POC_PHONE_EXT|  EMPLOYER_POC_EMAIL|AGENT_REPRESENTING_EMPLOYER|AGENT_ATTORNEY_LAST_NAME|AGENT_ATTORNEY_FIRST_NAME|AGENT_ATTORNEY_MIDDLE_NAME|AGENT_ATTORNEY_ADDRESS1|AGENT_ATTORNEY_ADDRESS2|AGENT_ATTORNEY_CITY|AGENT_ATTORNEY_STATE|AGENT_ATTORNEY_POSTAL_CODE|AGENT_ATTORNEY_COUNTRY|AGENT_ATTORNEY_PROVINCE|AGENT_ATTORNEY_PHONE|AGENT_ATTORNEY_PHONE_EXT|AGENT_ATTORNEY_EMAIL_ADDRESS|LAWFIRM_NAME_BUSINESS_NAME|STATE_OF_HIGHEST_COURT|NAME_OF_HIGHEST_STATE_COURT|WORKSITE_WORKERS|SECONDARY_ENTITY|SECONDARY_ENTITY_BUSINESS_NAME|   WORKSITE_ADDRESS1| WORKSITE_ADDRESS2| WORKSITE_CITY|WORKSITE_COUNTY|WORKSITE_STATE|WORKSITE_POSTAL_CODE|WAGE_RATE_OF_PAY_FROM|WAGE_RATE_OF_PAY_TO|WAGE_UNIT_OF_PAY|PREVAILING_WAGE|PW_UNIT_OF_PAY|PW_TRACKING_NUMBER|PW_WAGE_LEVEL|         PW_OES_YEAR|PW_OTHER_SOURCE|PW_OTHER_YEAR| PW_SURVEY_PUBLISHER|      PW_SURVEY_NAME|TOTAL_WORKSITE_LOCATIONS|AGREE_TO_LC_STATEMENT|H_1B_DEPENDENT|WILLFUL_VIOLATOR|SUPPORT_H1B|     STATUTORY_BASIS|APPENDIX_A_ATTACHED|PUBLIC_DISCLOSURE|PREPARER_LAST_NAME|PREPARER_FIRST_NAME|PREPARER_MIDDLE_INITIAL|PREPARER_BUSINESS_NAME|      PREPARER_EMAIL|\n",
            "+------------------+-----------+-------------+-------------+------------------+--------------+--------------------+----------+--------------------+------------------+----------+----------+----------------------+--------------+--------------------+--------------------------+-------------------------+---------------+----------------+--------------------+--------------+--------------------+-----------------+-------------+--------------+--------------------+--------------------+-----------------+--------------+------------------+----------+----------------------+-----------------------+------------------------+----------------------+---------------------+---------------------+-----------------+------------------+------------------------+--------------------+---------------------+------------------+----------------------+--------------------+---------------------------+------------------------+-------------------------+--------------------------+-----------------------+-----------------------+-------------------+--------------------+--------------------------+----------------------+-----------------------+--------------------+------------------------+----------------------------+--------------------------+----------------------+---------------------------+----------------+----------------+------------------------------+--------------------+------------------+--------------+---------------+--------------+--------------------+---------------------+-------------------+----------------+---------------+--------------+------------------+-------------+--------------------+---------------+-------------+--------------------+--------------------+------------------------+---------------------+--------------+----------------+-----------+--------------------+-------------------+-----------------+------------------+-------------------+-----------------------+----------------------+--------------------+\n",
            "|I-200-23355-584296|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|    Registered Nurse|29-1141.00|   Registered Nurses|                 Y|2023-12-21|2026-12-20|                     1|             1|                   0|                         0|                        0|              0|             0.0|Avant Healthcare ...|          NULL|     2301 Lucien Way|        Suite 360|     Maitland|            FL|               32751|UNITED STATES OF ...|             NULL|   14076812999|              NULL|    561320|                  Kaul|                 Saloni|                    NULL|  Director of Immig...|      2301 Lucien Way|            Suite 360|         Maitland|                FL|                   32751|UNITED STATES OF ...|                 NULL|       14076812999|                  NULL|skaul@avanthealth...|                        Yes|               Schneider|                    Maria|                        T.|   302 West Third St...|              Suite 710|         Cincinnati|                  OH|                     45202|  UNITED STATES OF ...|                   NULL|       15133818472.0|                    NULL|        tyler.peace@muimm...|      Musillo Unkenholt...|                    OH|       Supreme Court of ...|               1|             Yes|          Billings Clinic H...|2800 10th Avenue ...|              NULL|      Billings|    YELLOWSTONE|            MT|               59101|                35.42|               NULL|            Hour|          35.42|          Hour|              NULL|           II|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       1|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|             Peace|              Tyler|                      J|  Musillo Unkenholt...|tyler.peace@muimm...|\n",
            "|I-203-23355-583713|  Certified|   2023-12-21|   2023-12-29|              NULL|E-3 Australian|Infrastructure  E...|15-1244.00|Network and Compu...|                 Y|2023-12-21|2025-12-20|                     1|             0|                   1|                         0|                        0|              0|             0.0|TECHIE BRAINS INC...|          NULL|1713 FORT JESSE ROAD|           SUIT C|       NORMAL|            IL|               61761|UNITED STATES OF ...|             NULL|   19174766150|              NULL|    541511|             MADISETTY|                 NAVEEN|                    NULL|             PRESIDENT|        3602  como ct|                 NULL|           Normal|                IL|                   61761|UNITED STATES OF ...|                 NULL|       19174766150|                  NULL|NAVEEN@TECHIEBRAI...|                         No|                    NULL|                     NULL|                      NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                   NULL|                NULL|                    NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|             Yes|                   CVS  HEALTH|8300 NORMAN CENTE...|               800|   BLOOMINGTON|       HENNEPIN|            MN|               55437|              88712.0|               NULL|            Year|        81931.0|          Year|              NULL|           II|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       2|                  Yes|          NULL|            NULL|       NULL|                NULL|               NULL|Disclose Business|              NULL|               NULL|                   NULL|                  NULL|                NULL|\n",
            "|I-200-23355-584402|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|Sr. Lead - Qualit...|15-1253.00|Software Quality ...|                 Y|2024-04-01|2027-03-31|                     1|             0|                   1|                         0|                        0|              0|             0.0| ValueMomentum, Inc.|          NULL|220 Old New Bruns...|             NULL|   Piscataway|            NJ|               08854|UNITED STATES OF ...|             NULL|   19087550226|              NULL|     54151|                 Noria|                  Cyrus|                      R.|     Sr. Director - HR| 220 OLD NEW BRUNS...|                 NULL|       PISCATAWAY|                NJ|                   08854|UNITED STATES OF ...|                 NULL|       19087550105|                  NULL|CYRUS.NORIA@VALUE...|                        Yes|                     Box|                  Kaitlyn|                    Amanda|   ONE BATTERY PARK ...|              9TH FLOOR|           NEW YORK|                  NY|                     10004|  UNITED STATES OF ...|                   NULL|       12124250555.0|                    NULL|        KAITLYN@CYRUSMEHT...|      CYRUS D. MEHTA & ...|                    NY|       NEW YORK COURT OF...|               1|             Yes|          Erie Indemnity Co...|    125 E 6th Street|              NULL|          Erie|           ERIE|            PA|               16501|             133000.0|               NULL|            Year|        56493.0|          Year|              NULL|           II|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       2|                  Yes|           Yes|              No|        Yes|$60,000 or higher...|               NULL|Disclose Business|               Son|              Subin|                   NULL|  Cyrus D. Mehta & ...|subin@cyrusmehta.com|\n",
            "|I-200-23355-585360|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|AVP, Oracle ERP P...|15-1252.00| Software Developers|                 Y|2024-03-23|2027-03-22|                     1|             0|                   1|                         0|                        0|              0|             0.0|   JRI America, Inc.|          NULL|     277 Park Avenue|             NULL|     New York|            NY|               10172|UNITED STATES OF ...|             NULL|   12122244238|              NULL|     54151|                Becker|            Christopher|                    NULL|  Exec. Director, H...|      277 Park Avenue|                 NULL|         New York|                NY|                   10172|UNITED STATES OF ...|                 NULL|       12122244238|                  NULL|alexandra.mercado...|                        Yes|                 DRENNAN|                  MELISSA|                     BELLE|   1177 AVENUE OF TH...|             23RD FLOOR|           NEW YORK|                  NY|                     10036|  UNITED STATES OF ...|                     NY|       12127157554.0|                    NULL|        MDRENNAN@KRAMERLE...|      Kramer Levin Naft...|                    NY|              SUPREME COURT|               1|              No|                          NULL|Harborside 2, 200...|              NULL|   Jersey City|         HUDSON|            NJ|               07311|             130000.0|           150000.0|            Year|       127867.0|          Year|              NULL|         NULL|                NULL|         Survey|       2023.0|Willis Towers Watson|Gen. Industry Pro...|                       2|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|               Han|             Kristy|                   NULL|  Kramer Levin Naft...|khan@kramerlevin.com|\n",
            "|I-200-23355-584372|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|Database Administ...|15-1242.00|Database Administ...|                 Y|2024-06-18|2027-06-17|                     1|             1|                   0|                         0|                        0|              0|             0.0|Ernst & Young U.S...|          NULL|     200 Plaza Drive|             NULL|     Secaucus|            NJ|               07094|UNITED STATES OF ...|             NULL|   12018722200|               0.0|    541211|            Rubenstein|                 Sunday|                       D|  Associate Directo...|      200 Plaza Drive|                 NULL|         Secaucus|                NJ|                   07094|UNITED STATES OF ...|                 NULL|       12018723003|                   0.0|US.Visaimmigratio...|                        Yes|                Torriano|                  Jessica|                 Alexandra|   100 Adelaide Stre...|               Floor 31|            Toronto|                NULL|                    M5H0B3|                CANADA|                Ontario|       14169433837.0|                    NULL|        certified.lca@ca....|                EY Law LLP|                    IL|       Supreme Court of ...|               1|              No|                          NULL|  1 Harborside Place|           Apt 601|   Jersey City|         HUDSON|            NJ|               07311|             112000.0|               NULL|            Year|        93226.0|          Year|              NULL|           II|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       1|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|         Jesudasan|           Phylicia|                   NULL|            EY Law LLP|   niveyus@ca.ey.com|\n",
            "|I-200-23355-585496|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|Postdoctoral Rese...|19-2031.00|            Chemists|                 Y|2024-03-01|2025-02-28|                     1|             1|                   0|                         0|                        0|              0|             0.0|Carnegie Mellon U...|          NULL|  5000 Forbes Avenue|             NULL|   Pittsburgh|            PA|               15213|UNITED STATES OF ...|             NULL|   14122685231|              NULL|    611310|               Gentile|                  Linda|                    NULL|  Director, Office ...|   5000 Forbes Avenue| Office of Interna...|       Pittsburgh|                PA|                   15213|UNITED STATES OF ...|                 NULL|       14122685231|                  NULL|    lgentile@cmu.edu|                         No|                    NULL|                     NULL|                      NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                   NULL|                NULL|                    NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|              No|                          NULL|  5000 Forbes Avenue|Dept. of Chemistry|    Pittsburgh|      ALLEGHENY|            PA|               15213|              58000.0|               NULL|            Year|        48464.0|          Year|              NULL|            I|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       1|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|              NULL|               NULL|                   NULL|                  NULL|                NULL|\n",
            "|I-200-23355-584176|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|Product Manager I...|15-1299.09|Information Techn...|                 Y|2024-06-18|2027-06-16|                     1|             1|                   0|                         0|                        0|              0|             0.0|Amazon Developmen...|          NULL|     2250 7th Avenue|             NULL|      Seattle|            WA|               98121|UNITED STATES OF ...|             NULL|   12062661000|              NULL|    518210|       Gerling/Stankus|       Andreas/Jeremiah|                    NULL|        HR Immigration|      2250 7th Avenue|                 NULL|          Seattle|                WA|                   98121|UNITED STATES OF ...|                 NULL|       12062661000|                  NULL|amazonimmigration...|                         No|                    NULL|                     NULL|                      NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                   NULL|                NULL|                    NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|              No|                          NULL|2100 University Ave.|              NULL|East Palo Alto|      SAN MATEO|            CA|               94303|             194479.0|               NULL|            Year|       148450.0|          Year|              NULL|          III|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       2|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|               V R|           Trishika|                   NULL|  Amazon.com Servic...|amazonimmigration...|\n",
            "|I-200-23355-584001|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|SOFTWARE DEVELOPM...|15-1252.00| Software Developers|                 Y|2024-01-01|2026-12-31|                     1|             0|                   0|                         0|                        0|              1|             0.0|AMAZON.COM SERVIC...|          NULL|  1770 CRYSTAL DRIVE|             NULL|    ARLINGTON|            VA|               22202|UNITED STATES OF ...|             NULL|   12062661000|              NULL|    454110|        GERLING/GARCIA|           ANDREAS/ENIS|                    NULL|        HR IMMIGRATION|   1770 CRYSTAL DRIVE|                 NULL|        ARLINGTON|                VA|                   22202|UNITED STATES OF ...|                 NULL|       12062661000|                  NULL|AMAZONIMMIGRATION...|                        Yes|                  KENNEY|                     MARY|                      NULL|        100 HIGH STREET|    3RD FLOOR C#7925796|             BOSTON|                  MA|                     02110|  UNITED STATES OF ...|                   NULL|       16175740400.0|                    NULL|        AMAZONINDIA@FRAGO...|      FRAGOMEN, DEL REY...|                    MA|       SUPREME JUDICIAL ...|               1|              No|                          NULL|10300 CAMPUS POIN...|              NULL|     SAN DIEGO|      SAN DIEGO|            CA|               92121|             175000.0|               NULL|            Year|       121597.0|          Year|              NULL|           II|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       1|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|          VARGHESE|            SUJITHA|                   NULL|  FRAGOMEN, DEL REY...|amazonindia@frago...|\n",
            "|I-200-23355-585545|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|Business Systems ...|15-1211.00|Computer Systems ...|                 Y|2024-01-22|2027-01-21|                     1|             0|                   0|                         0|                        0|              1|             0.0|        Degree, Inc.|       Lattice|    360 Spear Street|          Floor 4|San Francisco|            CA|               94105|UNITED STATES OF ...|             NULL|   18888431972|              NULL|      5415|                Martin|                  Cheri|                    NULL|  Senior Director o...|     360 Spear Street|            4th Floor|    San Francisco|                CA|                   94105|UNITED STATES OF ...|                 NULL|       16508660339|                  NULL|cheri.martin@latt...|                        Yes|                    Bach|                    James|                    Arnold|        100 Bush Street|             Suite 1980|      San Francisco|                  CA|                     94104|  UNITED STATES OF ...|                   NULL|       14152483100.0|                    NULL|           jbach@immilaw.com|      Law Offices of Ja...|                    CA|       California Suprem...|               1|              No|                          NULL|4568 Woodlands Vi...|              NULL|       Orlando|         ORANGE|            FL|               32835|             121347.0|           155000.0|            Year|       121347.0|          Year|              NULL|           IV|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       1|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|       Fuson (HG1)|              Shawn|                      M|  Law Offices of Ja...|  sfuson@immilaw.com|\n",
            "|I-200-23355-584623|  Certified|   2023-12-21|   2023-12-29|              NULL|          H-1B|Data Center Infra...|17-2071.00|Electrical Engineers|                 Y|2024-01-02|2026-12-31|                     1|             0|                   0|                         0|                        0|              1|             0.0|Amazon Data Servi...|          NULL|     2250 7th Avenue|             NULL|      Seattle|            WA|               98121|UNITED STATES OF ...|       WASHINGTON|   12062661000|              NULL|    518210|       Gerling/Stankus|       Andreas/Jeremiah|                    NULL|        HR Immigration|      2250 7th Avenue|                 NULL|          Seattle|                WA|                   98121|UNITED STATES OF ...|                 NULL|       12062661000|                  NULL|amazonimmigration...|                         No|                    NULL|                     NULL|                      NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                   NULL|                NULL|                    NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|              No|                          NULL|  475 Sansome Street|              NULL| San Francisco|  SAN FRANCISCO|            CA|               94111|             169560.0|               NULL|            Year|       161221.0|          Year|              NULL|          III|7/1/2023 - 6/30/2024|           NULL|         NULL|                NULL|                NULL|                       2|                  Yes|            No|              No|       NULL|                NULL|               NULL|Disclose Business|         Raghuveer|            Koppela|                   NULL|  Amazon.com Servic...|amazonimmigration...|\n",
            "+------------------+-----------+-------------+-------------+------------------+--------------+--------------------+----------+--------------------+------------------+----------+----------+----------------------+--------------+--------------------+--------------------------+-------------------------+---------------+----------------+--------------------+--------------+--------------------+-----------------+-------------+--------------+--------------------+--------------------+-----------------+--------------+------------------+----------+----------------------+-----------------------+------------------------+----------------------+---------------------+---------------------+-----------------+------------------+------------------------+--------------------+---------------------+------------------+----------------------+--------------------+---------------------------+------------------------+-------------------------+--------------------------+-----------------------+-----------------------+-------------------+--------------------+--------------------------+----------------------+-----------------------+--------------------+------------------------+----------------------------+--------------------------+----------------------+---------------------------+----------------+----------------+------------------------------+--------------------+------------------+--------------+---------------+--------------+--------------------+---------------------+-------------------+----------------+---------------+--------------+------------------+-------------+--------------------+---------------+-------------+--------------------+--------------------+------------------------+---------------------+--------------+----------------+-----------+--------------------+-------------------+-----------------+------------------+-------------------+-----------------------+----------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read and union all CSVs using Spark\n",
        "# Initialize union DataFrame\n",
        "df_union = None\n",
        "\n",
        "# List all relevant CSV files, excluding the summary file\n",
        "files = [\n",
        "    f for f in os.listdir(path)\n",
        "    if f.endswith(\".csv\") and \"to_FY2024\" not in f\n",
        "]\n",
        "\n",
        "# Read and union all CSVs using Spark with schema inference\n",
        "for file in files:\n",
        "    file_path = f\"file://{os.path.join(path, file)}\"\n",
        "    print(\"Joining:\", file_path)\n",
        "\n",
        "    df = spark.read \\\n",
        "        .option(\"header\", True) \\\n",
        "        .option(\"inferSchema\", True) \\\n",
        "        .csv(file_path)\n",
        "\n",
        "    if df_union is None:\n",
        "        df_union = df\n",
        "    else:\n",
        "        df_union = df_union.unionByName(df)\n",
        "\n",
        "# Show result\n",
        "df_union.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxXDq0dFxZTP"
      },
      "source": [
        "# data cleaning and EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OpQ4P5_JxZTQ",
        "outputId": "0fa930f2-d099-4380-cbd3-513533d025ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CASE_NUMBER: string (nullable = true)\n",
            " |-- CASE_STATUS: string (nullable = true)\n",
            " |-- RECEIVED_DATE: date (nullable = true)\n",
            " |-- DECISION_DATE: date (nullable = true)\n",
            " |-- ORIGINAL_CERT_DATE: date (nullable = true)\n",
            " |-- VISA_CLASS: string (nullable = true)\n",
            " |-- JOB_TITLE: string (nullable = true)\n",
            " |-- SOC_CODE: string (nullable = true)\n",
            " |-- SOC_TITLE: string (nullable = true)\n",
            " |-- FULL_TIME_POSITION: string (nullable = true)\n",
            " |-- BEGIN_DATE: string (nullable = true)\n",
            " |-- END_DATE: string (nullable = true)\n",
            " |-- TOTAL_WORKER_POSITIONS: string (nullable = true)\n",
            " |-- NEW_EMPLOYMENT: string (nullable = true)\n",
            " |-- CONTINUED_EMPLOYMENT: string (nullable = true)\n",
            " |-- CHANGE_PREVIOUS_EMPLOYMENT: string (nullable = true)\n",
            " |-- NEW_CONCURRENT_EMPLOYMENT: string (nullable = true)\n",
            " |-- CHANGE_EMPLOYER: string (nullable = true)\n",
            " |-- AMENDED_PETITION: string (nullable = true)\n",
            " |-- EMPLOYER_NAME: string (nullable = true)\n",
            " |-- TRADE_NAME_DBA: string (nullable = true)\n",
            " |-- EMPLOYER_ADDRESS1: string (nullable = true)\n",
            " |-- EMPLOYER_ADDRESS2: string (nullable = true)\n",
            " |-- EMPLOYER_CITY: string (nullable = true)\n",
            " |-- EMPLOYER_STATE: string (nullable = true)\n",
            " |-- EMPLOYER_POSTAL_CODE: string (nullable = true)\n",
            " |-- EMPLOYER_COUNTRY: string (nullable = true)\n",
            " |-- EMPLOYER_PROVINCE: string (nullable = true)\n",
            " |-- EMPLOYER_PHONE: string (nullable = true)\n",
            " |-- EMPLOYER_PHONE_EXT: string (nullable = true)\n",
            " |-- NAICS_CODE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_LAST_NAME: string (nullable = true)\n",
            " |-- EMPLOYER_POC_FIRST_NAME: string (nullable = true)\n",
            " |-- EMPLOYER_POC_MIDDLE_NAME: string (nullable = true)\n",
            " |-- EMPLOYER_POC_JOB_TITLE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_ADDRESS1: string (nullable = true)\n",
            " |-- EMPLOYER_POC_ADDRESS2: string (nullable = true)\n",
            " |-- EMPLOYER_POC_CITY: string (nullable = true)\n",
            " |-- EMPLOYER_POC_STATE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_POSTAL_CODE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_COUNTRY: string (nullable = true)\n",
            " |-- EMPLOYER_POC_PROVINCE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_PHONE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_PHONE_EXT: string (nullable = true)\n",
            " |-- EMPLOYER_POC_EMAIL: string (nullable = true)\n",
            " |-- AGENT_REPRESENTING_EMPLOYER: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_LAST_NAME: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_FIRST_NAME: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_MIDDLE_NAME: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_ADDRESS1: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_ADDRESS2: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_CITY: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_STATE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_POSTAL_CODE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_COUNTRY: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_PROVINCE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_PHONE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_PHONE_EXT: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_EMAIL_ADDRESS: string (nullable = true)\n",
            " |-- LAWFIRM_NAME_BUSINESS_NAME: string (nullable = true)\n",
            " |-- STATE_OF_HIGHEST_COURT: string (nullable = true)\n",
            " |-- NAME_OF_HIGHEST_STATE_COURT: string (nullable = true)\n",
            " |-- WORKSITE_WORKERS: string (nullable = true)\n",
            " |-- SECONDARY_ENTITY: string (nullable = true)\n",
            " |-- SECONDARY_ENTITY_BUSINESS_NAME: string (nullable = true)\n",
            " |-- WORKSITE_ADDRESS1: string (nullable = true)\n",
            " |-- WORKSITE_ADDRESS2: string (nullable = true)\n",
            " |-- WORKSITE_CITY: string (nullable = true)\n",
            " |-- WORKSITE_COUNTY: string (nullable = true)\n",
            " |-- WORKSITE_STATE: string (nullable = true)\n",
            " |-- WORKSITE_POSTAL_CODE: string (nullable = true)\n",
            " |-- WAGE_RATE_OF_PAY_FROM: string (nullable = true)\n",
            " |-- WAGE_RATE_OF_PAY_TO: double (nullable = true)\n",
            " |-- WAGE_UNIT_OF_PAY: string (nullable = true)\n",
            " |-- PREVAILING_WAGE: string (nullable = true)\n",
            " |-- PW_UNIT_OF_PAY: string (nullable = true)\n",
            " |-- PW_TRACKING_NUMBER: string (nullable = true)\n",
            " |-- PW_WAGE_LEVEL: string (nullable = true)\n",
            " |-- PW_OES_YEAR: string (nullable = true)\n",
            " |-- PW_OTHER_SOURCE: string (nullable = true)\n",
            " |-- PW_OTHER_YEAR: string (nullable = true)\n",
            " |-- PW_SURVEY_PUBLISHER: string (nullable = true)\n",
            " |-- PW_SURVEY_NAME: string (nullable = true)\n",
            " |-- TOTAL_WORKSITE_LOCATIONS: string (nullable = true)\n",
            " |-- AGREE_TO_LC_STATEMENT: string (nullable = true)\n",
            " |-- H_1B_DEPENDENT: string (nullable = true)\n",
            " |-- WILLFUL_VIOLATOR: string (nullable = true)\n",
            " |-- SUPPORT_H1B: string (nullable = true)\n",
            " |-- STATUTORY_BASIS: string (nullable = true)\n",
            " |-- APPENDIX_A_ATTACHED: string (nullable = true)\n",
            " |-- PUBLIC_DISCLOSURE: string (nullable = true)\n",
            " |-- PREPARER_LAST_NAME: string (nullable = true)\n",
            " |-- PREPARER_FIRST_NAME: string (nullable = true)\n",
            " |-- PREPARER_MIDDLE_INITIAL: string (nullable = true)\n",
            " |-- PREPARER_BUSINESS_NAME: string (nullable = true)\n",
            " |-- PREPARER_EMAIL: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_union.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4Qdc_gO1xZTR",
        "outputId": "5b0602bf-e4f5-4d5f-9a52-b9306ea10a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 3564698\n",
            "Number of columns: 96\n"
          ]
        }
      ],
      "source": [
        "num_rows = df_union.count()\n",
        "num_columns = len(df_union.columns)\n",
        "\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uL8N3FKWxZTR",
        "outputId": "00727cfd-1b49-499e-f0b1-79b124b5748b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Numerical Columns: 1\n",
            "Number of String (Object) Columns: 92\n",
            "Number of Other Data Type Columns: 3\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Get the schema as a list of tuples (column_name, data_type)\n",
        "schema_info = df_union.dtypes\n",
        "\n",
        "# Initialize counters\n",
        "num_numeric = 0\n",
        "num_string = 0\n",
        "num_other = 0\n",
        "\n",
        "# Classify columns based on their data types\n",
        "for col_name, col_type in schema_info:\n",
        "    if col_type in ['int', 'bigint', 'double', 'float', 'decimal', 'long', 'short']:\n",
        "        num_numeric += 1\n",
        "    elif col_type in ['string']:\n",
        "        num_string += 1\n",
        "    else:\n",
        "        num_other += 1\n",
        "\n",
        "# Display the summary\n",
        "print(f\"Number of Numerical Columns: {num_numeric}\")\n",
        "print(f\"Number of String (Object) Columns: {num_string}\")\n",
        "print(f\"Number of Other Data Type Columns: {num_other}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmgwVGSXGWCW"
      },
      "source": [
        "### Adress missing data\n",
        "\n",
        "delete columns that have more than 50% null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CiB12cIHxZTS",
        "outputId": "f926eb04-8489-44f0-b9cb-f817885264ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------+-------------+-------------+--------------+--------------------+----------+--------------------+------------------+----------+----------+----------------------+--------------+--------------------+--------------------------+-------------------------+---------------+----------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------+----------+----------------------+-----------------------+----------------------+---------------------+-----------------+------------------+------------------------+--------------------+------------------+--------------------+---------------------------+------------------------+-------------------------+-----------------------+-----------------------+-------------------+--------------------+--------------------------+----------------------+--------------------+----------------------------+--------------------------+----------------------+---------------------------+----------------+----------------+--------------------+--------------+---------------+--------------+--------------------+---------------------+----------------+---------------+--------------+-------------+--------------------+------------------------+---------------------+--------------+----------------+-----------------+----------------------+--------------------+\n",
            "|       CASE_NUMBER|CASE_STATUS|RECEIVED_DATE|DECISION_DATE|    VISA_CLASS|           JOB_TITLE|  SOC_CODE|           SOC_TITLE|FULL_TIME_POSITION|BEGIN_DATE|  END_DATE|TOTAL_WORKER_POSITIONS|NEW_EMPLOYMENT|CONTINUED_EMPLOYMENT|CHANGE_PREVIOUS_EMPLOYMENT|NEW_CONCURRENT_EMPLOYMENT|CHANGE_EMPLOYER|AMENDED_PETITION|       EMPLOYER_NAME|   EMPLOYER_ADDRESS1|EMPLOYER_CITY|EMPLOYER_STATE|EMPLOYER_POSTAL_CODE|    EMPLOYER_COUNTRY|EMPLOYER_PHONE|NAICS_CODE|EMPLOYER_POC_LAST_NAME|EMPLOYER_POC_FIRST_NAME|EMPLOYER_POC_JOB_TITLE|EMPLOYER_POC_ADDRESS1|EMPLOYER_POC_CITY|EMPLOYER_POC_STATE|EMPLOYER_POC_POSTAL_CODE|EMPLOYER_POC_COUNTRY|EMPLOYER_POC_PHONE|  EMPLOYER_POC_EMAIL|AGENT_REPRESENTING_EMPLOYER|AGENT_ATTORNEY_LAST_NAME|AGENT_ATTORNEY_FIRST_NAME|AGENT_ATTORNEY_ADDRESS1|AGENT_ATTORNEY_ADDRESS2|AGENT_ATTORNEY_CITY|AGENT_ATTORNEY_STATE|AGENT_ATTORNEY_POSTAL_CODE|AGENT_ATTORNEY_COUNTRY|AGENT_ATTORNEY_PHONE|AGENT_ATTORNEY_EMAIL_ADDRESS|LAWFIRM_NAME_BUSINESS_NAME|STATE_OF_HIGHEST_COURT|NAME_OF_HIGHEST_STATE_COURT|WORKSITE_WORKERS|SECONDARY_ENTITY|   WORKSITE_ADDRESS1| WORKSITE_CITY|WORKSITE_COUNTY|WORKSITE_STATE|WORKSITE_POSTAL_CODE|WAGE_RATE_OF_PAY_FROM|WAGE_UNIT_OF_PAY|PREVAILING_WAGE|PW_UNIT_OF_PAY|PW_WAGE_LEVEL|         PW_OES_YEAR|TOTAL_WORKSITE_LOCATIONS|AGREE_TO_LC_STATEMENT|H_1B_DEPENDENT|WILLFUL_VIOLATOR|PUBLIC_DISCLOSURE|PREPARER_BUSINESS_NAME|      PREPARER_EMAIL|\n",
            "+------------------+-----------+-------------+-------------+--------------+--------------------+----------+--------------------+------------------+----------+----------+----------------------+--------------+--------------------+--------------------------+-------------------------+---------------+----------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------+----------+----------------------+-----------------------+----------------------+---------------------+-----------------+------------------+------------------------+--------------------+------------------+--------------------+---------------------------+------------------------+-------------------------+-----------------------+-----------------------+-------------------+--------------------+--------------------------+----------------------+--------------------+----------------------------+--------------------------+----------------------+---------------------------+----------------+----------------+--------------------+--------------+---------------+--------------+--------------------+---------------------+----------------+---------------+--------------+-------------+--------------------+------------------------+---------------------+--------------+----------------+-----------------+----------------------+--------------------+\n",
            "|I-200-23355-584296|  Certified|   2023-12-21|   2023-12-29|          H-1B|    Registered Nurse|29-1141.00|   Registered Nurses|                 Y|2023-12-21|2026-12-20|                     1|             1|                   0|                         0|                        0|              0|             0.0|Avant Healthcare ...|     2301 Lucien Way|     Maitland|            FL|               32751|UNITED STATES OF ...|   14076812999|    561320|                  Kaul|                 Saloni|  Director of Immig...|      2301 Lucien Way|         Maitland|                FL|                   32751|UNITED STATES OF ...|       14076812999|skaul@avanthealth...|                        Yes|               Schneider|                    Maria|   302 West Third St...|              Suite 710|         Cincinnati|                  OH|                     45202|  UNITED STATES OF ...|       15133818472.0|        tyler.peace@muimm...|      Musillo Unkenholt...|                    OH|       Supreme Court of ...|               1|             Yes|2800 10th Avenue ...|      Billings|    YELLOWSTONE|            MT|               59101|                35.42|            Hour|          35.42|          Hour|           II|7/1/2023 - 6/30/2024|                       1|                  Yes|            No|              No|Disclose Business|  Musillo Unkenholt...|tyler.peace@muimm...|\n",
            "|I-203-23355-583713|  Certified|   2023-12-21|   2023-12-29|E-3 Australian|Infrastructure  E...|15-1244.00|Network and Compu...|                 Y|2023-12-21|2025-12-20|                     1|             0|                   1|                         0|                        0|              0|             0.0|TECHIE BRAINS INC...|1713 FORT JESSE ROAD|       NORMAL|            IL|               61761|UNITED STATES OF ...|   19174766150|    541511|             MADISETTY|                 NAVEEN|             PRESIDENT|        3602  como ct|           Normal|                IL|                   61761|UNITED STATES OF ...|       19174766150|NAVEEN@TECHIEBRAI...|                         No|                    NULL|                     NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|             Yes|8300 NORMAN CENTE...|   BLOOMINGTON|       HENNEPIN|            MN|               55437|              88712.0|            Year|        81931.0|          Year|           II|7/1/2023 - 6/30/2024|                       2|                  Yes|          NULL|            NULL|Disclose Business|                  NULL|                NULL|\n",
            "|I-200-23355-584402|  Certified|   2023-12-21|   2023-12-29|          H-1B|Sr. Lead - Qualit...|15-1253.00|Software Quality ...|                 Y|2024-04-01|2027-03-31|                     1|             0|                   1|                         0|                        0|              0|             0.0| ValueMomentum, Inc.|220 Old New Bruns...|   Piscataway|            NJ|               08854|UNITED STATES OF ...|   19087550226|     54151|                 Noria|                  Cyrus|     Sr. Director - HR| 220 OLD NEW BRUNS...|       PISCATAWAY|                NJ|                   08854|UNITED STATES OF ...|       19087550105|CYRUS.NORIA@VALUE...|                        Yes|                     Box|                  Kaitlyn|   ONE BATTERY PARK ...|              9TH FLOOR|           NEW YORK|                  NY|                     10004|  UNITED STATES OF ...|       12124250555.0|        KAITLYN@CYRUSMEHT...|      CYRUS D. MEHTA & ...|                    NY|       NEW YORK COURT OF...|               1|             Yes|    125 E 6th Street|          Erie|           ERIE|            PA|               16501|             133000.0|            Year|        56493.0|          Year|           II|7/1/2023 - 6/30/2024|                       2|                  Yes|           Yes|              No|Disclose Business|  Cyrus D. Mehta & ...|subin@cyrusmehta.com|\n",
            "|I-200-23355-585360|  Certified|   2023-12-21|   2023-12-29|          H-1B|AVP, Oracle ERP P...|15-1252.00| Software Developers|                 Y|2024-03-23|2027-03-22|                     1|             0|                   1|                         0|                        0|              0|             0.0|   JRI America, Inc.|     277 Park Avenue|     New York|            NY|               10172|UNITED STATES OF ...|   12122244238|     54151|                Becker|            Christopher|  Exec. Director, H...|      277 Park Avenue|         New York|                NY|                   10172|UNITED STATES OF ...|       12122244238|alexandra.mercado...|                        Yes|                 DRENNAN|                  MELISSA|   1177 AVENUE OF TH...|             23RD FLOOR|           NEW YORK|                  NY|                     10036|  UNITED STATES OF ...|       12127157554.0|        MDRENNAN@KRAMERLE...|      Kramer Levin Naft...|                    NY|              SUPREME COURT|               1|              No|Harborside 2, 200...|   Jersey City|         HUDSON|            NJ|               07311|             130000.0|            Year|       127867.0|          Year|         NULL|                NULL|                       2|                  Yes|            No|              No|Disclose Business|  Kramer Levin Naft...|khan@kramerlevin.com|\n",
            "|I-200-23355-584372|  Certified|   2023-12-21|   2023-12-29|          H-1B|Database Administ...|15-1242.00|Database Administ...|                 Y|2024-06-18|2027-06-17|                     1|             1|                   0|                         0|                        0|              0|             0.0|Ernst & Young U.S...|     200 Plaza Drive|     Secaucus|            NJ|               07094|UNITED STATES OF ...|   12018722200|    541211|            Rubenstein|                 Sunday|  Associate Directo...|      200 Plaza Drive|         Secaucus|                NJ|                   07094|UNITED STATES OF ...|       12018723003|US.Visaimmigratio...|                        Yes|                Torriano|                  Jessica|   100 Adelaide Stre...|               Floor 31|            Toronto|                NULL|                    M5H0B3|                CANADA|       14169433837.0|        certified.lca@ca....|                EY Law LLP|                    IL|       Supreme Court of ...|               1|              No|  1 Harborside Place|   Jersey City|         HUDSON|            NJ|               07311|             112000.0|            Year|        93226.0|          Year|           II|7/1/2023 - 6/30/2024|                       1|                  Yes|            No|              No|Disclose Business|            EY Law LLP|   niveyus@ca.ey.com|\n",
            "|I-200-23355-585496|  Certified|   2023-12-21|   2023-12-29|          H-1B|Postdoctoral Rese...|19-2031.00|            Chemists|                 Y|2024-03-01|2025-02-28|                     1|             1|                   0|                         0|                        0|              0|             0.0|Carnegie Mellon U...|  5000 Forbes Avenue|   Pittsburgh|            PA|               15213|UNITED STATES OF ...|   14122685231|    611310|               Gentile|                  Linda|  Director, Office ...|   5000 Forbes Avenue|       Pittsburgh|                PA|                   15213|UNITED STATES OF ...|       14122685231|    lgentile@cmu.edu|                         No|                    NULL|                     NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|              No|  5000 Forbes Avenue|    Pittsburgh|      ALLEGHENY|            PA|               15213|              58000.0|            Year|        48464.0|          Year|            I|7/1/2023 - 6/30/2024|                       1|                  Yes|            No|              No|Disclose Business|                  NULL|                NULL|\n",
            "|I-200-23355-584176|  Certified|   2023-12-21|   2023-12-29|          H-1B|Product Manager I...|15-1299.09|Information Techn...|                 Y|2024-06-18|2027-06-16|                     1|             1|                   0|                         0|                        0|              0|             0.0|Amazon Developmen...|     2250 7th Avenue|      Seattle|            WA|               98121|UNITED STATES OF ...|   12062661000|    518210|       Gerling/Stankus|       Andreas/Jeremiah|        HR Immigration|      2250 7th Avenue|          Seattle|                WA|                   98121|UNITED STATES OF ...|       12062661000|amazonimmigration...|                         No|                    NULL|                     NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|              No|2100 University Ave.|East Palo Alto|      SAN MATEO|            CA|               94303|             194479.0|            Year|       148450.0|          Year|          III|7/1/2023 - 6/30/2024|                       2|                  Yes|            No|              No|Disclose Business|  Amazon.com Servic...|amazonimmigration...|\n",
            "|I-200-23355-584001|  Certified|   2023-12-21|   2023-12-29|          H-1B|SOFTWARE DEVELOPM...|15-1252.00| Software Developers|                 Y|2024-01-01|2026-12-31|                     1|             0|                   0|                         0|                        0|              1|             0.0|AMAZON.COM SERVIC...|  1770 CRYSTAL DRIVE|    ARLINGTON|            VA|               22202|UNITED STATES OF ...|   12062661000|    454110|        GERLING/GARCIA|           ANDREAS/ENIS|        HR IMMIGRATION|   1770 CRYSTAL DRIVE|        ARLINGTON|                VA|                   22202|UNITED STATES OF ...|       12062661000|AMAZONIMMIGRATION...|                        Yes|                  KENNEY|                     MARY|        100 HIGH STREET|    3RD FLOOR C#7925796|             BOSTON|                  MA|                     02110|  UNITED STATES OF ...|       16175740400.0|        AMAZONINDIA@FRAGO...|      FRAGOMEN, DEL REY...|                    MA|       SUPREME JUDICIAL ...|               1|              No|10300 CAMPUS POIN...|     SAN DIEGO|      SAN DIEGO|            CA|               92121|             175000.0|            Year|       121597.0|          Year|           II|7/1/2023 - 6/30/2024|                       1|                  Yes|            No|              No|Disclose Business|  FRAGOMEN, DEL REY...|amazonindia@frago...|\n",
            "|I-200-23355-585545|  Certified|   2023-12-21|   2023-12-29|          H-1B|Business Systems ...|15-1211.00|Computer Systems ...|                 Y|2024-01-22|2027-01-21|                     1|             0|                   0|                         0|                        0|              1|             0.0|        Degree, Inc.|    360 Spear Street|San Francisco|            CA|               94105|UNITED STATES OF ...|   18888431972|      5415|                Martin|                  Cheri|  Senior Director o...|     360 Spear Street|    San Francisco|                CA|                   94105|UNITED STATES OF ...|       16508660339|cheri.martin@latt...|                        Yes|                    Bach|                    James|        100 Bush Street|             Suite 1980|      San Francisco|                  CA|                     94104|  UNITED STATES OF ...|       14152483100.0|           jbach@immilaw.com|      Law Offices of Ja...|                    CA|       California Suprem...|               1|              No|4568 Woodlands Vi...|       Orlando|         ORANGE|            FL|               32835|             121347.0|            Year|       121347.0|          Year|           IV|7/1/2023 - 6/30/2024|                       1|                  Yes|            No|              No|Disclose Business|  Law Offices of Ja...|  sfuson@immilaw.com|\n",
            "|I-200-23355-584623|  Certified|   2023-12-21|   2023-12-29|          H-1B|Data Center Infra...|17-2071.00|Electrical Engineers|                 Y|2024-01-02|2026-12-31|                     1|             0|                   0|                         0|                        0|              1|             0.0|Amazon Data Servi...|     2250 7th Avenue|      Seattle|            WA|               98121|UNITED STATES OF ...|   12062661000|    518210|       Gerling/Stankus|       Andreas/Jeremiah|        HR Immigration|      2250 7th Avenue|          Seattle|                WA|                   98121|UNITED STATES OF ...|       12062661000|amazonimmigration...|                         No|                    NULL|                     NULL|                   NULL|                   NULL|               NULL|                NULL|                      NULL|                  NULL|                NULL|                        NULL|                      NULL|                  NULL|                       NULL|               1|              No|  475 Sansome Street| San Francisco|  SAN FRANCISCO|            CA|               94111|             169560.0|            Year|       161221.0|          Year|          III|7/1/2023 - 6/30/2024|                       2|                  Yes|            No|              No|Disclose Business|  Amazon.com Servic...|amazonimmigration...|\n",
            "+------------------+-----------+-------------+-------------+--------------+--------------------+----------+--------------------+------------------+----------+----------+----------------------+--------------+--------------------+--------------------------+-------------------------+---------------+----------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------+----------+----------------------+-----------------------+----------------------+---------------------+-----------------+------------------+------------------------+--------------------+------------------+--------------------+---------------------------+------------------------+-------------------------+-----------------------+-----------------------+-------------------+--------------------+--------------------------+----------------------+--------------------+----------------------------+--------------------------+----------------------+---------------------------+----------------+----------------+--------------------+--------------+---------------+--------------+--------------------+---------------------+----------------+---------------+--------------+-------------+--------------------+------------------------+---------------------+--------------+----------------+-----------------+----------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Null values (percentage)- delete columns that have more than 50% null values\n",
        "from pyspark.sql.functions import col, sum, when\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "num_rows = df_union.count()\n",
        "null_count_df = df_union.select([\n",
        "    (sum(when(col(c).isNull(), 1).otherwise(0)) / num_rows * 100).alias(c)\n",
        "    for c in df_union.columns\n",
        "])\n",
        "\n",
        "# Collect the missing percentage as a dictionary\n",
        "missing_percentage = null_count_df.collect()[0].asDict()\n",
        "\n",
        "# Filter out columns with more than 50% missing values\n",
        "columns_to_keep = [col_name for col_name, perc in missing_percentage.items() if perc <= 50]\n",
        "\n",
        "# Create a new DataFrame with the filtered columns\n",
        "filtered_df = df_union.select(columns_to_keep)\n",
        "\n",
        "# Display the first few rows to verify the content\n",
        "filtered_df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH1Dl8h5GWCX"
      },
      "source": [
        "#### Filter out only the full time employee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RH2J1mbRGWCX",
        "outputId": "09897e2f-0794-4547-bf86-767397fd7c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+----------+\n",
            "|  FULL_TIME_POSITION|  count|percentage|\n",
            "+--------------------+-------+----------+\n",
            "|                NULL|     59|       0.0|\n",
            "|                   Y|3506603|     98.37|\n",
            "| Materials Engineers|      3|       0.0|\n",
            "|                   N|  57978|      1.63|\n",
            "|Health Specialtie...|      1|       0.0|\n",
            "|000 or higher in ...|     26|       0.0|\n",
            "|  Financial Analysts|      1|       0.0|\n",
            "|Software Develope...|      3|       0.0|\n",
            "|                LLC\"|      2|       0.0|\n",
            "|000 or higher ann...|     12|       0.0|\n",
            "|               Ren 2|      2|       0.0|\n",
            "|               INC.\"|      2|       0.0|\n",
            "|Environmental Eng...|      1|       0.0|\n",
            "|\",Rancho Cordova,...|      1|       0.0|\n",
            "| Department of Ch...|      1|       0.0|\n",
            "|Regulatory Affair...|      1|       0.0|\n",
            "|          Suite 100\"|      1|       0.0|\n",
            "|          27-1014.00|      1|       0.0|\n",
            "+--------------------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check how many people are full time\n",
        "from pyspark.sql.functions import col, count, round\n",
        "\n",
        "# Count total number of rows\n",
        "total_count = filtered_df.count()\n",
        "\n",
        "# Count Y and N values, and calculate their percentage\n",
        "filtered_df.groupBy(\"FULL_TIME_POSITION\") \\\n",
        "    .agg(count(\"*\").alias(\"count\")) \\\n",
        "    .withColumn(\"percentage\", round((col(\"count\") / total_count) * 100, 2)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iXMfrr4wGWCX"
      },
      "outputs": [],
      "source": [
        "# Filter only rows where the FULL_TIME_POSITION value is 'Y'\n",
        "df_full_time_only = filtered_df.filter(col(\"FULL_TIME_POSITION\") == \"Y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhe1w44aGWCY"
      },
      "source": [
        "#### Calculate years of experience as one of our features for predictive modeling using the 'begin_date' and 'end_date' columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "55Ddf5acGWCY",
        "outputId": "42152ada-c772-41f3-b6a2-ff0a8ec98b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------------+\n",
            "|BEGIN_DATE|  END_DATE|yrs_of_experience|\n",
            "+----------+----------+-----------------+\n",
            "|2023-12-21|2026-12-20|              3.0|\n",
            "|2023-12-21|2025-12-20|              2.0|\n",
            "|2024-04-01|2027-03-31|              3.0|\n",
            "|2024-03-23|2027-03-22|              3.0|\n",
            "|2024-06-18|2027-06-17|              3.0|\n",
            "|2024-03-01|2025-02-28|              1.0|\n",
            "|2024-06-18|2027-06-16|             2.99|\n",
            "|2024-01-01|2026-12-31|              3.0|\n",
            "|2024-01-22|2027-01-21|              3.0|\n",
            "|2024-01-02|2026-12-31|              3.0|\n",
            "|2023-12-29|2026-12-28|              3.0|\n",
            "|2024-01-08|2027-01-07|              3.0|\n",
            "|2023-12-21|2026-12-20|              3.0|\n",
            "|2024-03-23|2027-03-22|              3.0|\n",
            "|2023-12-25|2026-12-24|              3.0|\n",
            "|2024-06-18|2027-06-17|              3.0|\n",
            "|2024-01-01|2026-12-31|              3.0|\n",
            "|2024-01-18|2027-01-17|              3.0|\n",
            "|2024-06-19|2027-06-18|              3.0|\n",
            "|2024-06-07|2027-06-06|              3.0|\n",
            "+----------+----------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate years of experience\n",
        "from pyspark.sql.functions import to_date, datediff, round, col\n",
        "\n",
        "# Step 1: Parse ISO date strings and filter rows with missing dates\n",
        "df_dates = df_full_time_only.withColumn(\"BEGIN_DATE_PARSED\", to_date(col(\"BEGIN_DATE\"))) \\\n",
        "                   .withColumn(\"END_DATE_PARSED\", to_date(col(\"END_DATE\"))) \\\n",
        "                   .filter(col(\"BEGIN_DATE_PARSED\").isNotNull() & col(\"END_DATE_PARSED\").isNotNull())\n",
        "\n",
        "# Step 2: Calculate experience in years (rounded to 2 decimals)\n",
        "df_dates = df_dates.withColumn(\"yrs_of_experience\",\n",
        "                               round(datediff(col(\"END_DATE_PARSED\"), col(\"BEGIN_DATE_PARSED\")) / 365.0, 2))\n",
        "\n",
        "# Step 3: Show result\n",
        "df_dates.select(\"BEGIN_DATE\", \"END_DATE\", \"yrs_of_experience\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmgplZ7MGWCY"
      },
      "source": [
        "#### Categorize job titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h0yVn5WqGWCY",
        "outputId": "765e969e-6248-4863-8a1f-9d587f20542d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|           SOC_TITLE|           Job_Group|\n",
            "+--------------------+--------------------+\n",
            "|   Registered Nurses|          Healthcare|\n",
            "|Network and Compu...|    Computer Science|\n",
            "|Software Quality ...|    Computer Science|\n",
            "| Software Developers|    Computer Science|\n",
            "|Database Administ...|                Data|\n",
            "|            Chemists|        Bio-chemical|\n",
            "|Information Techn...|    Computer Science|\n",
            "| Software Developers|    Computer Science|\n",
            "|Computer Systems ...|    Computer Science|\n",
            "|Electrical Engineers|    Computer Science|\n",
            "|Mechanical Engineers|Production, Const...|\n",
            "| Software Developers|    Computer Science|\n",
            "|Computer User Sup...|    Computer Science|\n",
            "|Computer and Info...|    Computer Science|\n",
            "|Financial and Inv...|Business & Accoun...|\n",
            "|Accountants and A...|Business & Accoun...|\n",
            "| Software Developers|    Computer Science|\n",
            "|Financial Quantit...|Business & Accoun...|\n",
            "|Mechanical Engineers|Production, Const...|\n",
            "|Computer Systems ...|    Computer Science|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define your categories as a list of tuples (already correct)\n",
        "categories = [\n",
        "    (\"Computer Science\", [\"software\", \"developer\", \"computer\", \"programmer\", \"programming\", \"architect\", \"elect\", \"information\",\"security\"]),\n",
        "    (\"Business & Accounting\", [\"account\", \"audit\",\"tax\",\"business\", \"finan\", \"market\", \"purchas\", \"supply chain\", \"estate\", \"broker\", \"cost\", \"budget\", \"buyer\", \"management\", \"sale\", \"risk\", \"invest\", \"operation\", \"econ\", \"human resour\", \"credit\", \"manager\", \"project manager\",\"intelligence\",\"trader\",\"treasu\",\"talent\",\"insur\"]),\n",
        "    (\"Data\", [\"data\", \"statistic\"]),\n",
        "    (\"Law\", [\"law\", \"police\", \"political\", \"regulat\", \"compliance\",'legal']),\n",
        "    (\"Healthcare\", [\"medical\", \"physician\", \"doctor\", \"nurse\", \"therap\", \"animal\", \"surgeon\", \"health\", \"cardiovascular\", \"clini\", \"hospital\", \"dentist\", \"pediatri\", \"pharm\", \"epidemio\", \"dental\",\"internist\",\"psychiatrist\",\"vet\",\"fitness\",\"patient\",\"opticians\",\"prosthodontist\"]),\n",
        "    (\"Bio-chemical\", [\"bio\", \"chem\", \"plastic\", \"nuclear\", \"materi\", \"natural\"]),\n",
        "    (\"Art/commu\", [\"design\", \"singer\", \"designer\", \"art\", \"audio\", \"jewel\", \"radio\", \"media\", \"public relation\", \"relig\", \"music\", \"actor\",\"producer\",\"dance\",\"film\",\"model\",\"choreographer\",\"photo\"]),\n",
        "    (\"Higher Management\", [\"chief\", \"vice president\", \"vp\"]),\n",
        "    (\"Education\", [\"education\", \"educator\", \"tutor\", \"drafter\", \"pathologist\", \"teacher\", \"train\", \"instruction\", \"professor\",\"learn\",\"translat\"]),\n",
        "    (\"Production, Construction,& Manufacturing\", [\"production\", \"assembler\", \"fabricator\", \"quality control\", \"operator\", \"machinist\",\"mechan\",\"repair\", \"machine\", \"manufacturing\", \"millwright\", \"weld\",\"civil\", \"architect\", \"maintenance\", \"construct\", \"floor\",\"technician\"]),\n",
        "    (\"Geo & Transportation\", [\"pilot\", \"captain\", \"vessel\", \"driver\", \"transportation\", \"truck\", \"courier\",\"logistic\",\"freight\",\"transport\",\"city\",\"geo\"]),\n",
        "    (\"Agriculture & Forestry & environment\", [\"agricultural\", \"forester\", \"conservation\", \"horticulture\", \"farming\", \"agronomist\",\"landscaping\", \"groundskeeping\",\"sustain\", \"envir\", \"soil\",\"recyc\",\"tree\",\"climate\"]),\n",
        "    (\"Office & Administrative\", [\"office\", \"administrative\", \"clerk\", \"secretary\", \"procurement\", \"payroll\", \"file\",\"clerical\", \"customer service\", \"assistant\",\"editor\",\"writer\"]),\n",
        "    (\"Social & Community Services\", [\"social\", \"counselor\", \"therapist\", \"community\", \"child\", \"residential\",\"coach\", \"scout\", \"rehabilitation\", \"community service\",\"firefighter\", \"protective\", \"coroner\",\"food\", \"librar\", \"museum\", \"bake\", \"travel\", \"hotel\", \"cook\",\"planner\",\"family\",\"curators\",\"clergy\",\"concierges\",\"entertain\"]),\n",
        "    (\"Sciences\", [\"physicist\", \"mathematician\", \"actuary\", \"astronomer\", \"geoscientist\", \"hydrologist\",\"genetic\", \"social scientist\", \"anthropologist\", \"archaeologist\", \"historian\",\"remote sensing\", \"conservation\", \"cytogenetic\", \"cytotechnologist\",\"scientist\",\"ists\",\"research\",\"actuaries\",\"recreat\"]),\n",
        "    (\"engineer\", [\"engineer\"])\n",
        "]\n",
        "\n",
        "# Convert list of tuples to a UDF-compatible function\n",
        "def assign_category(title):\n",
        "    if title is None:\n",
        "        return \"Other\"\n",
        "    title_lower = title.lower()\n",
        "    for category, keywords in categories:\n",
        "        for keyword in keywords:\n",
        "            if keyword in title_lower:\n",
        "                return category\n",
        "    return \"Other\"\n",
        "\n",
        "# Register the function as a UDF\n",
        "assign_category_udf = udf(assign_category, StringType())\n",
        "\n",
        "# Apply the UDF to create a new column 'Job_Group'\n",
        "df_with_group = df_dates.withColumn(\"Job_Group\", assign_category_udf(col(\"SOC_TITLE\")))\n",
        "\n",
        "# Remove rows where Job_Group is \"Other\"\n",
        "df_with_group = df_with_group.filter(col(\"Job_Group\") != \"Other\")\n",
        "\n",
        "# Show a sample\n",
        "df_with_group.select(\"SOC_TITLE\", \"Job_Group\").show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa-UgCM2GWCZ"
      },
      "source": [
        "### remove outliers from wage column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9A8M31fMGWCZ"
      },
      "outputs": [],
      "source": [
        "# filter only H1-B visa\n",
        "df_h1b = df_with_group.filter(col(\"VISA_CLASS\") == \"H-1B\")\n",
        "\n",
        "# filter certified H1-B\n",
        "df_certified = df_h1b.filter(col(\"CASE_STATUS\") == \"Certified\")\n",
        "\n",
        "# filter out null values in states\n",
        "df_cleaned = df_certified.filter(\n",
        "    col(\"WORKSITE_STATE\").isNotNull() & col(\"EMPLOYER_STATE\").isNotNull()\n",
        ")\n",
        "\n",
        "# Cast the wage column to FloatType\n",
        "df_cleaned = df_cleaned.withColumn(\"WAGE_RATE_OF_PAY_FROM\", col(\"WAGE_RATE_OF_PAY_FROM\").cast(\"float\"))\n",
        "\n",
        "# filter out null values in wage column\n",
        "df_cleaned = df_cleaned.filter(col(\"WAGE_RATE_OF_PAY_FROM\").isNotNull())\n",
        "\n",
        "# filter out yearly pay < 60k\n",
        "df_cleaned = df_cleaned.filter(col(\"WAGE_UNIT_OF_PAY\") == \"Year\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0VK2DPbbGWCZ",
        "outputId": "dcf7524d-2e0c-420a-97bf-f44c30705cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lower Bound: 9000.0, Upper Bound: 213000.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate IQR (Q1 and Q3)\n",
        "quantiles = df_cleaned.approxQuantile(\"WAGE_RATE_OF_PAY_FROM\", [0.25, 0.75], 0.05)\n",
        "q1, q3 = quantiles\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Define bounds\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_jTyzB8BGWCZ"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df_cleaned.filter(\n",
        "    (col(\"WAGE_RATE_OF_PAY_FROM\") >= 60000) & (col(\"WAGE_RATE_OF_PAY_FROM\") <= upper_bound)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u3iQxE_MGWCZ",
        "outputId": "43da2670-c867-4c3d-d0f6-6ace68e0ae61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+\n",
            "|summary|WAGE_RATE_OF_PAY_FROM|\n",
            "+-------+---------------------+\n",
            "|  count|              2800139|\n",
            "|    min|              60000.0|\n",
            "|    max|             213000.0|\n",
            "+-------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_cleaned.select(\"WAGE_RATE_OF_PAY_FROM\").summary(\"count\", \"min\", \"max\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GWaxFXF-F4k"
      },
      "source": [
        "### detrend the salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3WtLG8wH-F4k"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year, col, avg, broadcast\n",
        "\n",
        "# Step 1: Extract the year from RECEIVED_DATE\n",
        "df_with_year = df_cleaned.withColumn(\"YEAR\", year(col(\"RECEIVED_DATE\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iiR0cjxl-F4k",
        "outputId": "d0440945-6fdf-4ac0-fc0a-d4236116049c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---------------------+------------------+\n",
            "|           Job_Group|YEAR|WAGE_RATE_OF_PAY_FROM|    WAGE_DETRENDED|\n",
            "+--------------------+----+---------------------+------------------+\n",
            "|    Computer Science|2023|             133000.0|119649.47147181997|\n",
            "|    Computer Science|2023|             130000.0|116950.61121305711|\n",
            "|                Data|2023|             112000.0| 96262.31232656725|\n",
            "|    Computer Science|2023|             194479.0| 174957.2147546472|\n",
            "|    Computer Science|2023|             175000.0|157433.51509449995|\n",
            "|    Computer Science|2023|             121347.0|109166.19860669877|\n",
            "|    Computer Science|2023|             169560.0|152539.58182527663|\n",
            "|Production, Const...|2023|             131500.0|120057.04808719583|\n",
            "|    Computer Science|2023|             112000.0|100757.44966047998|\n",
            "|    Computer Science|2023|             100848.0| 90724.88645857217|\n",
            "|    Computer Science|2023|             169776.0|152733.89976390757|\n",
            "|Business & Accoun...|2023|            191999.77|180738.70018646985|\n",
            "|Business & Accoun...|2023|              72750.0| 68483.10671507198|\n",
            "|    Computer Science|2023|             140000.0|125946.81207559997|\n",
            "|Business & Accoun...|2023|            106863.35|  100595.660604604|\n",
            "|Production, Const...|2023|              96304.0|   87923.756342124|\n",
            "|    Computer Science|2023|             177000.0|159232.75526700853|\n",
            "|                Data|2023|            175890.94|151175.61036640822|\n",
            "|    Computer Science|2023|             197800.0|177944.85306109767|\n",
            "|          Healthcare|2023|              65000.0| 68788.96014280937|\n",
            "+--------------------+----+---------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Calculate mean wage for each (Job_Group, YEAR)\n",
        "mean_wages = df_with_year.groupBy(\"Job_Group\", \"YEAR\") \\\n",
        "    .agg(avg(\"WAGE_RATE_OF_PAY_FROM\").alias(\"mean_wage\"))\n",
        "\n",
        "# Step 3: Filter out 2019 mean wages as the baseline\n",
        "baseline_2019 = mean_wages.filter(col(\"YEAR\") == 2019) \\\n",
        "    .select(\"Job_Group\", col(\"mean_wage\").alias(\"baseline_wage\"))\n",
        "\n",
        "# Step 4: Join 2019 baseline wages back to all year-group mean wages\n",
        "mean_wages_with_baseline = mean_wages.join(\n",
        "    broadcast(baseline_2019), on=\"Job_Group\", how=\"inner\"\n",
        ")\n",
        "\n",
        "# Step 5: Compute multiplier = baseline / current year mean\n",
        "mean_wages_with_baseline = mean_wages_with_baseline.withColumn(\n",
        "    \"multiplier\", col(\"baseline_wage\") / col(\"mean_wage\")\n",
        ")\n",
        "\n",
        "# Step 6: Join multiplier back to main data on (Job_Group, YEAR)\n",
        "df_detrend_ready = df_with_year.join(\n",
        "    broadcast(mean_wages_with_baseline.select(\"Job_Group\", \"YEAR\", \"multiplier\")),\n",
        "    on=[\"Job_Group\", \"YEAR\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Step 7: Apply the multiplier to get detrended wage\n",
        "df_detrended = df_detrend_ready.withColumn(\n",
        "    \"WAGE_DETRENDED\", col(\"WAGE_RATE_OF_PAY_FROM\") * col(\"multiplier\")\n",
        ")\n",
        "\n",
        "# Preview the result\n",
        "df_detrended.select(\"Job_Group\", \"YEAR\", \"WAGE_RATE_OF_PAY_FROM\", \"WAGE_DETRENDED\").show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0G4MUQtGWCZ"
      },
      "source": [
        "### add wage bucket\n",
        "- df_cleaned is the df after clean.\n",
        "- df_bucket is the df after adding wage bucket.\n",
        "We add 2 columns: Wage Bucket (Categorical Label) and wage_bucket (Numerical Range). In detail, Wage Bucket represents a human-readable label for the wage range. wage_bucket shows the actual numerical range corresponding to the wage bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jibKeoi-GWCa"
      },
      "outputs": [],
      "source": [
        "# df_buckets = df_buckets.drop(\"Wage Bucket\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0jxYwiSbGWCa",
        "outputId": "7100078c-64e4-4607-bda6-91903ccb126e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|Wage Bucket|     wage_bucket| Count|\n",
            "+-----------+----------------+------+\n",
            "|  100k-120k|(100000, 120000)|566317|\n",
            "|  120k-140k|(120000, 140000)|433006|\n",
            "|  140k-160k|(140000, 160000)|303666|\n",
            "|  160k-180k|(160000, 180000)|199786|\n",
            "|  180k-200k|(180000, 200000)|118603|\n",
            "|  200k-220k|(200000, 220000)| 54383|\n",
            "|    60k-80k|  (60000, 80000)|437496|\n",
            "|   80k-100k| (80000, 100000)|686882|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, when, lit, count\n",
        "\n",
        "# Define bucket ranges and labels\n",
        "bucket_intervals = [(60000, 80000), (80000, 100000), (100000, 120000), (120000, 140000),\n",
        "                    (140000, 160000), (160000, 180000), (180000, 200000), (200000, 220000),\n",
        "                    (220000, 240000)]\n",
        "bucket_labels = [f\"{lower//1000}k-{upper//1000}k\" for lower, upper in bucket_intervals]\n",
        "\n",
        "# Start with an empty \"Wage Bucket\" column\n",
        "df_buckets = df_detrended.withColumn(\"Wage Bucket\", lit(None)).withColumn(\"wage_bucket\", lit(None))\n",
        "\n",
        "# Use when conditions to assign the correct bucket\n",
        "for i, ((lower, upper), label) in enumerate(zip(bucket_intervals, bucket_labels)):\n",
        "    if i < len(bucket_intervals) - 1:\n",
        "        # For all but the last bucket, use exclusive upper bound\n",
        "        df_buckets = df_buckets.withColumn(\n",
        "            \"Wage Bucket\",\n",
        "            when((col(\"WAGE_RATE_OF_PAY_FROM\") >= lower) & (col(\"WAGE_RATE_OF_PAY_FROM\") < upper), label)\n",
        "            .otherwise(col(\"Wage Bucket\"))\n",
        "        ).withColumn(\n",
        "            \"wage_bucket\",\n",
        "            when((col(\"WAGE_RATE_OF_PAY_FROM\") >= lower) & (col(\"WAGE_RATE_OF_PAY_FROM\") < upper), f\"({lower}, {upper})\")\n",
        "            .otherwise(col(\"wage_bucket\"))\n",
        "        )\n",
        "    else:\n",
        "        # For the last bucket, use inclusive upper bound\n",
        "        df_buckets = df_buckets.withColumn(\n",
        "            \"Wage Bucket\",\n",
        "            when((col(\"WAGE_RATE_OF_PAY_FROM\") >= lower) & (col(\"WAGE_RATE_OF_PAY_FROM\") <= upper), label)\n",
        "            .otherwise(col(\"Wage Bucket\"))\n",
        "        ).withColumn(\n",
        "            \"wage_bucket\",\n",
        "            when((col(\"WAGE_RATE_OF_PAY_FROM\") >= lower) & (col(\"WAGE_RATE_OF_PAY_FROM\") <= upper), f\"({lower}, {upper})\")\n",
        "            .otherwise(col(\"wage_bucket\"))\n",
        "        )\n",
        "\n",
        "# Handle any values that did not fall into the defined ranges\n",
        "df_buckets = df_buckets.withColumn(\n",
        "    \"Wage Bucket\",\n",
        "    when(col(\"Wage Bucket\").isNull(), \"Out of Range\").otherwise(col(\"Wage Bucket\"))\n",
        ").withColumn(\n",
        "    \"wage_bucket\",\n",
        "    when(col(\"wage_bucket\").isNull(), \"Out of Range\").otherwise(col(\"wage_bucket\"))\n",
        ")\n",
        "\n",
        "# Group by bucket and count occurrences\n",
        "bucket_counts = df_buckets.groupBy(\"Wage Bucket\", \"wage_bucket\").agg(count(\"*\").alias(\"Count\")).orderBy(\"Wage Bucket\")\n",
        "\n",
        "# Show the result\n",
        "bucket_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "onX1TGhiGWCa",
        "outputId": "92630719-2b7c-40b5-f76f-f61a1091c8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CASE_NUMBER: string (nullable = true)\n",
            " |-- CASE_STATUS: string (nullable = true)\n",
            " |-- RECEIVED_DATE: date (nullable = true)\n",
            " |-- DECISION_DATE: date (nullable = true)\n",
            " |-- VISA_CLASS: string (nullable = true)\n",
            " |-- JOB_TITLE: string (nullable = true)\n",
            " |-- SOC_CODE: string (nullable = true)\n",
            " |-- SOC_TITLE: string (nullable = true)\n",
            " |-- FULL_TIME_POSITION: string (nullable = true)\n",
            " |-- BEGIN_DATE: string (nullable = true)\n",
            " |-- END_DATE: string (nullable = true)\n",
            " |-- TOTAL_WORKER_POSITIONS: string (nullable = true)\n",
            " |-- NEW_EMPLOYMENT: string (nullable = true)\n",
            " |-- CONTINUED_EMPLOYMENT: string (nullable = true)\n",
            " |-- CHANGE_PREVIOUS_EMPLOYMENT: string (nullable = true)\n",
            " |-- NEW_CONCURRENT_EMPLOYMENT: string (nullable = true)\n",
            " |-- CHANGE_EMPLOYER: string (nullable = true)\n",
            " |-- AMENDED_PETITION: string (nullable = true)\n",
            " |-- EMPLOYER_NAME: string (nullable = true)\n",
            " |-- EMPLOYER_ADDRESS1: string (nullable = true)\n",
            " |-- EMPLOYER_CITY: string (nullable = true)\n",
            " |-- EMPLOYER_STATE: string (nullable = true)\n",
            " |-- EMPLOYER_POSTAL_CODE: string (nullable = true)\n",
            " |-- EMPLOYER_COUNTRY: string (nullable = true)\n",
            " |-- EMPLOYER_PHONE: string (nullable = true)\n",
            " |-- NAICS_CODE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_LAST_NAME: string (nullable = true)\n",
            " |-- EMPLOYER_POC_FIRST_NAME: string (nullable = true)\n",
            " |-- EMPLOYER_POC_JOB_TITLE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_ADDRESS1: string (nullable = true)\n",
            " |-- EMPLOYER_POC_CITY: string (nullable = true)\n",
            " |-- EMPLOYER_POC_STATE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_POSTAL_CODE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_COUNTRY: string (nullable = true)\n",
            " |-- EMPLOYER_POC_PHONE: string (nullable = true)\n",
            " |-- EMPLOYER_POC_EMAIL: string (nullable = true)\n",
            " |-- AGENT_REPRESENTING_EMPLOYER: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_LAST_NAME: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_FIRST_NAME: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_ADDRESS1: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_ADDRESS2: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_CITY: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_STATE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_POSTAL_CODE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_COUNTRY: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_PHONE: string (nullable = true)\n",
            " |-- AGENT_ATTORNEY_EMAIL_ADDRESS: string (nullable = true)\n",
            " |-- LAWFIRM_NAME_BUSINESS_NAME: string (nullable = true)\n",
            " |-- STATE_OF_HIGHEST_COURT: string (nullable = true)\n",
            " |-- NAME_OF_HIGHEST_STATE_COURT: string (nullable = true)\n",
            " |-- WORKSITE_WORKERS: string (nullable = true)\n",
            " |-- SECONDARY_ENTITY: string (nullable = true)\n",
            " |-- WORKSITE_ADDRESS1: string (nullable = true)\n",
            " |-- WORKSITE_CITY: string (nullable = true)\n",
            " |-- WORKSITE_COUNTY: string (nullable = true)\n",
            " |-- WORKSITE_STATE: string (nullable = true)\n",
            " |-- WORKSITE_POSTAL_CODE: string (nullable = true)\n",
            " |-- WAGE_RATE_OF_PAY_FROM: float (nullable = true)\n",
            " |-- WAGE_UNIT_OF_PAY: string (nullable = true)\n",
            " |-- PREVAILING_WAGE: string (nullable = true)\n",
            " |-- PW_UNIT_OF_PAY: string (nullable = true)\n",
            " |-- PW_WAGE_LEVEL: string (nullable = true)\n",
            " |-- PW_OES_YEAR: string (nullable = true)\n",
            " |-- TOTAL_WORKSITE_LOCATIONS: string (nullable = true)\n",
            " |-- AGREE_TO_LC_STATEMENT: string (nullable = true)\n",
            " |-- H_1B_DEPENDENT: string (nullable = true)\n",
            " |-- WILLFUL_VIOLATOR: string (nullable = true)\n",
            " |-- PUBLIC_DISCLOSURE: string (nullable = true)\n",
            " |-- PREPARER_BUSINESS_NAME: string (nullable = true)\n",
            " |-- PREPARER_EMAIL: string (nullable = true)\n",
            " |-- BEGIN_DATE_PARSED: date (nullable = true)\n",
            " |-- END_DATE_PARSED: date (nullable = true)\n",
            " |-- yrs_of_experience: double (nullable = true)\n",
            " |-- Job_Group: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_cleaned.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YpWZZrNGGWCa"
      },
      "outputs": [],
      "source": [
        "# # Set checkpoint directory\n",
        "# CHECKPOINT_PATH = \"file:///tmp/spark_checkpoints\"\n",
        "# spark.sparkContext.setCheckpointDir(CHECKPOINT_PATH)\n",
        "\n",
        "# df_cleaned = df_cleaned.checkpoint(eager=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4aP437vGWCa"
      },
      "source": [
        "### raw features and derived features planed to use:\n",
        "begin_date, end_date, years_of_experience, wage, soc_title, job category, worksite_state, employer_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icBJ6R_FGWCa"
      },
      "source": [
        "### Plot of Distribution of Wage\n",
        "\n",
        "This histogram displays the distribution of wage for certified H-1B visa positions, reflecting the offered annual salaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pPNIVu93GWCb"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Step 1: Convert to Pandas (only the needed column)\n",
        "# wages_pd = df_cleaned.select(\"WAGE_RATE_OF_PAY_FROM\") \\\n",
        "#                     .toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2_piODdwGWCb"
      },
      "outputs": [],
      "source": [
        "# # Step 2: Plot using seaborn\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.distplot(wages_pd[\"WAGE_RATE_OF_PAY_FROM\"], bins=50, kde=True, color='skyblue')\n",
        "# plt.title(\"Distribution of WAGE_RATE_OF_PAY_FROM\")\n",
        "# plt.xlabel(\"Wage ($)\")\n",
        "# plt.ylabel(\"Count\")\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4dDiH0NGWCb"
      },
      "source": [
        "#### Plot of Average Wage by Job Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oIYpkHV9GWCb"
      },
      "outputs": [],
      "source": [
        "# # Convert needed columns from PySpark to Pandas\n",
        "# wages_group_pd = df_cleaned.select(\"WAGE_RATE_OF_PAY_FROM\", \"Job_Group\") \\\n",
        "#     .toPandas()\n",
        "\n",
        "# # Group and compute mean wage\n",
        "# avg_wage_by_group = wages_group_pd.groupby(\"Job_Group\")[\"WAGE_RATE_OF_PAY_FROM\"].mean().sort_values(ascending=False)\n",
        "\n",
        "# # Plot\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.barplot(x=avg_wage_by_group.index, y=avg_wage_by_group.values, palette=\"coolwarm\")\n",
        "# plt.xticks(rotation=45, ha=\"right\")\n",
        "# plt.title(\"Average Wage by Job Category (H-1B Certified)\")\n",
        "# plt.xlabel(\"Job Category\")\n",
        "# plt.ylabel(\"Average Wage ($)\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSujPHGgGWCb"
      },
      "source": [
        "## Model: Linear Regression\n",
        "\n",
        "## Objective\n",
        "\n",
        "- **Predict:** `WAGE_RATE_OF_PAY_FROM` (salary)\n",
        "- **Based on:** Job category, years of experience, and region (`WORKSITE_STATE`)\n",
        "\n",
        "---\n",
        "\n",
        "## Model Overview\n",
        "\n",
        "- **Applied Linear Regression:**\n",
        "  - Captured linear relationships between salary and predictors.\n",
        "  - Evaluated predictor significance using statistical tests (p-values, coefficients).\n",
        "\n",
        "- **Implementation Steps:**\n",
        "  1. **Feature Selection & Preprocessing:**\n",
        "     - Selected features: `Job_Group`, `yrs_of_experience`, `WORKSITE_STATE`\n",
        "     - Encoded categorical variables using `StringIndexer` and `OneHotEncoder`\n",
        "  2. **Data Splitting:**\n",
        "     - Split dataset into 80% training and 20% testing subsets.\n",
        "  3. **Model Fitting:**\n",
        "     - Fitted the linear regression model on the training data.\n",
        "  4. **Model Evaluation:**\n",
        "     - Assessed predictive performance using R², RMSE, and MAE on both training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9RvPK5plGWCc"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "# from pyspark.ml.regression import LinearRegression\n",
        "# from pyspark.ml import Pipeline\n",
        "# from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "# from pyspark.sql.functions import when, col\n",
        "\n",
        "# # Initialize Spark session\n",
        "# spark = SparkSession.builder.appName(\"H1BLinearRegression\").getOrCreate()\n",
        "\n",
        "# # Load the dataset (here df_cleaned is assumed to be your pre-loaded Spark DataFrame)\n",
        "# data = df_cleaned\n",
        "\n",
        "# # Select relevant columns (make sure these columns exist in your DataFrame)\n",
        "# data = data.select(\"Job_Group\", \"yrs_of_experience\", \"WORKSITE_STATE\", \"WAGE_RATE_OF_PAY_FROM\")\n",
        "\n",
        "# # Define categorical columns (for this example, we use Job_Group, WORKSITE_STATE, and EMPLOYER_STATE)\n",
        "# categorical_columns = [\"Job_Group\", \"WORKSITE_STATE\"]\n",
        "\n",
        "# # Create StringIndexers for categorical columns\n",
        "# indexers = [\n",
        "#     StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
        "#     for col in categorical_columns\n",
        "# ]\n",
        "\n",
        "# # Create OneHotEncoders for the indexed columns\n",
        "# encoders = [\n",
        "#     OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\")\n",
        "#     for indexer in indexers\n",
        "# ]\n",
        "\n",
        "# # Assemble features: encoded categorical columns plus numeric column \"yrs_of_experience\"\n",
        "# assembler_inputs = [col + \"_index_encoded\" for col in categorical_columns] + [\"yrs_of_experience\"]\n",
        "# assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "# # Define the Linear Regression model\n",
        "# lr = LinearRegression(featuresCol=\"features\", labelCol=\"WAGE_RATE_OF_PAY_FROM\")\n",
        "\n",
        "# print(\"here\")\n",
        "\n",
        "# # Build the pipeline with indexers, encoders, assembler, and the regression model\n",
        "# pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# # Fit the pipeline model on the training data\n",
        "# model = pipeline.fit(train)\n",
        "\n",
        "# print(\"here\")\n",
        "\n",
        "# # Make predictions on both the training and test sets\n",
        "# train_predictions = model.transform(train)\n",
        "# test_predictions = model.transform(test)\n",
        "\n",
        "# # -------------------------\n",
        "# # Regression Metrics\n",
        "# # -------------------------\n",
        "\n",
        "# # Define RegressionEvaluators for RMSE, MAE, and R²\n",
        "# reg_evaluator_rmse = RegressionEvaluator(labelCol=\"WAGE_RATE_OF_PAY_FROM\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "# reg_evaluator_mae = RegressionEvaluator(labelCol=\"WAGE_RATE_OF_PAY_FROM\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "# reg_evaluator_r2  = RegressionEvaluator(labelCol=\"WAGE_RATE_OF_PAY_FROM\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "# # Evaluate on training set\n",
        "# print(\"Regression Metrics on Training Data:\")\n",
        "# train_rmse = reg_evaluator_rmse.evaluate(train_predictions)\n",
        "# print(\"  RMSE:\", train_rmse)\n",
        "# train_mae  = reg_evaluator_mae.evaluate(train_predictions)\n",
        "# print(\"  MAE :\", train_mae)\n",
        "# train_r2   = reg_evaluator_r2.evaluate(train_predictions)\n",
        "# print(\"  R²  :\", train_r2)\n",
        "\n",
        "# # Evaluate on test set\n",
        "# test_rmse = reg_evaluator_rmse.evaluate(test_predictions)\n",
        "# test_mae  = reg_evaluator_mae.evaluate(test_predictions)\n",
        "# test_r2   = reg_evaluator_r2.evaluate(test_predictions)\n",
        "\n",
        "# print(\"\\nRegression Metrics on Test Data:\")\n",
        "# print(\"  RMSE:\", test_rmse)\n",
        "# print(\"  MAE :\", test_mae)\n",
        "# print(\"  R²  :\", test_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sHWuWIqGWCc"
      },
      "source": [
        "## Performance Metrics\n",
        "\n",
        "**Training Data:**\n",
        "- **RMSE:** 34,943.61  \n",
        "- **MAE:** 27,123.98  \n",
        "- **R²:** 0.205\n",
        "\n",
        "**Test Data:**\n",
        "- **RMSE:** 34,913.56  \n",
        "- **MAE:** 27,105.75  \n",
        "- **R²:** 0.203\n",
        "\n",
        "---\n",
        "\n",
        "## Interpretation\n",
        "\n",
        "- **RMSE & MAE:**  \n",
        "  - Represent the average error in salary predictions.\n",
        "- **R² (~20%):**  \n",
        "  - Indicates that approximately 20% of the variance in salary is explained by the current predictors.\n",
        "  - Suggests potential for model improvement by incorporating additional features or exploring alternative modeling techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geb-reGyGWCi"
      },
      "source": [
        "## Retrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VgZPGrpYGWCj"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "# from pyspark.ml.regression import LinearRegression\n",
        "# from pyspark.ml import Pipeline\n",
        "# from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "# from pyspark.sql.functions import when, col\n",
        "\n",
        "# # Initialize Spark session\n",
        "# spark = SparkSession.builder.appName(\"H1BLinearRegression\").getOrCreate()\n",
        "\n",
        "# # Load the dataset (here df_buckets is the one with wage bucket)\n",
        "# data = df_buckets\n",
        "\n",
        "# # Select relevant columns (make sure these columns exist in your DataFrame)\n",
        "# data = data.select(\"Job_Group\", \"yrs_of_experience\", \"WORKSITE_STATE\", \"wage_bucket\")\n",
        "\n",
        "# # Define categorical columns (for this example, we use Job_Group, WORKSITE_STATE, and EMPLOYER_STATE)\n",
        "# categorical_columns = [\"Job_Group\", \"WORKSITE_STATE\"]\n",
        "\n",
        "# # Create StringIndexers for categorical columns\n",
        "# indexers = [\n",
        "#     StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
        "#     for col in categorical_columns\n",
        "# ]\n",
        "\n",
        "# # Create OneHotEncoders for the indexed columns\n",
        "# encoders = [\n",
        "#     OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\")\n",
        "#     for indexer in indexers\n",
        "# ]\n",
        "\n",
        "# # Assemble features: encoded categorical columns plus numeric column \"yrs_of_experience\"\n",
        "# assembler_inputs = [col + \"_index_encoded\" for col in categorical_columns] + [\"yrs_of_experience\"]\n",
        "# assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "# # Define the Linear Regression model\n",
        "# lr = LinearRegression(featuresCol=\"features\", labelCol=\"wage_bucket\")\n",
        "\n",
        "# print(\"here\")\n",
        "\n",
        "# # Build the pipeline with indexers, encoders, assembler, and the regression model\n",
        "# pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# # Fit the pipeline model on the training data\n",
        "# model = pipeline.fit(train)\n",
        "\n",
        "# print(\"here\")\n",
        "\n",
        "# # Make predictions on both the training and test sets\n",
        "# train_predictions = model.transform(train)\n",
        "# test_predictions = model.transform(test)\n",
        "\n",
        "# # -------------------------\n",
        "# # Regression Metrics\n",
        "# # -------------------------\n",
        "\n",
        "# # Define RegressionEvaluators for RMSE, MAE, and R²\n",
        "# reg_evaluator_rmse = RegressionEvaluator(labelCol=\"wage_bucket\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "# reg_evaluator_mae = RegressionEvaluator(labelCol=\"wage_bucket\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "# reg_evaluator_r2  = RegressionEvaluator(labelCol=\"wage_bucket\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "# # Evaluate on training set\n",
        "# print(\"Regression Metrics on Training Data:\")\n",
        "# train_rmse = reg_evaluator_rmse.evaluate(train_predictions)\n",
        "# print(\"  RMSE:\", train_rmse)\n",
        "# train_mae  = reg_evaluator_mae.evaluate(train_predictions)\n",
        "# print(\"  MAE :\", train_mae)\n",
        "# train_r2   = reg_evaluator_r2.evaluate(train_predictions)\n",
        "# print(\"  R²  :\", train_r2)\n",
        "\n",
        "# # Evaluate on test set\n",
        "# print(\"\\nRegression Metrics on Test Data:\")\n",
        "# test_rmse = reg_evaluator_rmse.evaluate(test_predictions)\n",
        "# print(\"  RMSE:\", test_rmse)\n",
        "# test_mae  = reg_evaluator_mae.evaluate(test_predictions)\n",
        "# print(\"  MAE :\", test_mae)\n",
        "# test_r2   = reg_evaluator_r2.evaluate(test_predictions)\n",
        "# print(\"  R²  :\", test_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK7lMX3ZGWCj"
      },
      "source": [
        "# Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "B5Kyqt2UGWCj"
      },
      "outputs": [],
      "source": [
        "# column_names = df_cleaned.columns\n",
        "# print(column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKEqGfQE-F4s"
      },
      "source": [
        "## 1. Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfXVLXAYGWCj"
      },
      "source": [
        "### adress non-numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wDBgfy4nGWCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ade9e141-b406-4c32-9867-529a286d9db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+-----------------+--------------+-------------------------+--------------+-------------------------+\n",
            "|Job_Group                               |Job_Group_Encoded|EMPLOYER_STATE|Employer_Location_Encoded|WORKSITE_STATE|Worksite_Location_Encoded|\n",
            "+----------------------------------------+-----------------+--------------+-------------------------+--------------+-------------------------+\n",
            "|Computer Science                        |(15,[0],[1.0])   |NJ            |(56,[2],[1.0])           |PA            |(54,[9],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |NY            |(56,[4],[1.0])           |NJ            |(54,[4],[1.0])           |\n",
            "|Data                                    |(15,[2],[1.0])   |NJ            |(56,[2],[1.0])           |NJ            |(54,[4],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |WA            |(56,[3],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |VA            |(56,[9],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |FL            |(54,[10],[1.0])          |\n",
            "|Computer Science                        |(15,[0],[1.0])   |WA            |(56,[3],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Production, Construction,& Manufacturing|(15,[3],[1.0])   |CA            |(56,[0],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |MO            |(56,[17],[1.0])          |MO            |(54,[20],[1.0])          |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |OR            |(54,[21],[1.0])          |\n",
            "+----------------------------------------+-----------------+--------------+-------------------------+--------------+-------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. string index and one-hot encode each nominal feature\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# 1. Encode 'Job_Group'\n",
        "job_indexer = StringIndexer(inputCol=\"Job_Group\", outputCol=\"Job_Group_Index\")\n",
        "df_job_indexed = job_indexer.fit(df_cleaned).transform(df_cleaned)\n",
        "\n",
        "job_encoder = OneHotEncoder(inputCol=\"Job_Group_Index\", outputCol=\"Job_Group_Encoded\")\n",
        "df_job_encoded = job_encoder.fit(df_job_indexed).transform(df_job_indexed)\n",
        "\n",
        "# 2. Encode 'EMPLOYER_STATE'\n",
        "employer_indexer = StringIndexer(inputCol=\"EMPLOYER_STATE\", outputCol=\"Employer_Location_Index\")\n",
        "df_employer_indexed = employer_indexer.fit(df_job_encoded).transform(df_job_encoded)\n",
        "\n",
        "employer_encoder = OneHotEncoder(inputCol=\"Employer_Location_Index\", outputCol=\"Employer_Location_Encoded\")\n",
        "df_employer_encoded = employer_encoder.fit(df_employer_indexed).transform(df_employer_indexed)\n",
        "\n",
        "# 3. Encode 'WORKSITE_STATE'\n",
        "worksite_indexer = StringIndexer(inputCol=\"WORKSITE_STATE\", outputCol=\"Worksite_Location_Index\")\n",
        "df_worksite_indexed = worksite_indexer.fit(df_employer_encoded).transform(df_employer_encoded)\n",
        "\n",
        "worksite_encoder = OneHotEncoder(inputCol=\"Worksite_Location_Index\", outputCol=\"Worksite_Location_Encoded\")\n",
        "final_encoded_df = worksite_encoder.fit(df_worksite_indexed).transform(df_worksite_indexed)\n",
        "\n",
        "# Show encoded result (optional: adjust number of rows)\n",
        "final_encoded_df.select(\"Job_Group\", \"Job_Group_Encoded\",\n",
        "                        \"EMPLOYER_STATE\", \"Employer_Location_Encoded\",\n",
        "                        \"WORKSITE_STATE\", \"Worksite_Location_Encoded\").show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JB74heCVGWCk"
      },
      "outputs": [],
      "source": [
        "# 2. bundle all the 4 input features together\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Bundle the encoded features into a single vector column\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Job_Group_Encoded\", \"Employer_Location_Encoded\", \"Worksite_Location_Encoded\", \"yrs_of_experience\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Apply the assembler to the final encoded DataFrame\n",
        "vectorized_df = assembler.transform(final_encoded_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kKna1_9YGWCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2258ddb4-ac6e-4b42-ef38-e363504af058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------------+\n",
            "|            features|WAGE_RATE_OF_PAY_FROM|\n",
            "+--------------------+---------------------+\n",
            "|(126,[0,17,80,125...|             133000.0|\n",
            "|(126,[0,19,75,125...|             130000.0|\n",
            "|(126,[2,17,75,125...|             112000.0|\n",
            "|(126,[0,18,71,125...|             194479.0|\n",
            "|(126,[0,24,71,125...|             175000.0|\n",
            "+--------------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. choose the data\n",
        "final_data = vectorized_df.select(\"features\",'WAGE_RATE_OF_PAY_FROM')\n",
        "final_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0_yKEDM_GWCk"
      },
      "outputs": [],
      "source": [
        "# 4. split the train and test data\n",
        "train_data,test_data = final_data.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-bPR98NfGWCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "373ccddc-3254-4636-c45e-609619e6ec04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:20:19] Starting training: Decision Tree Regressor...\n",
            "[2025-04-12 19:20:19] Fitting the Decision Tree model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Decision Tree: 100%|██████████| 1/1 [01:20<00:00, 80.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:21:40] Training completed: Decision Tree Regressor.\n",
            "[2025-04-12 19:21:40] Predicting with Decision Tree Regressor...\n",
            "[2025-04-12 19:21:40] Prediction completed: Decision Tree Regressor.\n",
            "[2025-04-12 19:21:40] Starting training: Random Forest Regressor...\n",
            "[2025-04-12 19:21:40] Fitting the Random Forest model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Random Forest: 100%|██████████| 1/1 [01:40<00:00, 100.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:23:21] Training completed: Random Forest Regressor.\n",
            "[2025-04-12 19:23:21] Predicting with Random Forest Regressor...\n",
            "[2025-04-12 19:23:21] Prediction completed: Random Forest Regressor.\n",
            "[2025-04-12 19:23:21] Starting training: Gradient-Boosted Tree Regressor...\n",
            "[2025-04-12 19:23:21] Fitting the Gradient-Boosted Tree model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Gradient-Boosted Tree: 100%|██████████| 1/1 [07:10<00:00, 430.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:30:31] Training completed: Gradient-Boosted Tree Regressor.\n",
            "[2025-04-12 19:30:31] Predicting with Gradient-Boosted Tree Regressor...\n",
            "[2025-04-12 19:30:31] Prediction completed: Gradient-Boosted Tree Regressor.\n",
            "[2025-04-12 19:30:31] All models trained and predictions made successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor, RandomForestRegressor\n",
        "from pyspark.sql import SparkSession\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# 5. train the model\n",
        "def print_progress(message):\n",
        "    \"\"\"Helper function to print progress with a timestamp.\"\"\"\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Step 1: Train Decision Tree Regressor\n",
        "print_progress(\"Starting training: Decision Tree Regressor...\")\n",
        "dtr = DecisionTreeRegressor(labelCol='WAGE_RATE_OF_PAY_FROM', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Decision Tree model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Decision Tree\"):\n",
        "    dtr_model = dtr.fit(train_data)\n",
        "print_progress(\"Training completed: Decision Tree Regressor.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Decision Tree Regressor...\")\n",
        "dtr_predictions = dtr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Decision Tree Regressor.\")\n",
        "\n",
        "# Step 2: Train Random Forest Regressor\n",
        "print_progress(\"Starting training: Random Forest Regressor...\")\n",
        "rfr = RandomForestRegressor(labelCol='WAGE_RATE_OF_PAY_FROM', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Random Forest model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Random Forest\"):\n",
        "    rfr_model = rfr.fit(train_data)\n",
        "print_progress(\"Training completed: Random Forest Regressor.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Random Forest Regressor...\")\n",
        "rfr_predictions = rfr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Random Forest Regressor.\")\n",
        "\n",
        "# Step 3: Train Gradient-Boosted Tree Regressor\n",
        "print_progress(\"Starting training: Gradient-Boosted Tree Regressor...\")\n",
        "gbtr = GBTRegressor(labelCol='WAGE_RATE_OF_PAY_FROM', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Gradient-Boosted Tree model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Gradient-Boosted Tree\"):\n",
        "    gbtr_model = gbtr.fit(train_data)\n",
        "print_progress(\"Training completed: Gradient-Boosted Tree Regressor.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Gradient-Boosted Tree Regressor...\")\n",
        "gbtr_predictions = gbtr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Gradient-Boosted Tree Regressor.\")\n",
        "\n",
        "print_progress(\"All models trained and predictions made successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "457Mc4DtGWCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "da5b84e6-0bf9-4759-f42c-336ce9f10c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the results!\n",
            "--------------------------------------------------------------------------------\n",
            "A single decision tree had an RMSE of: 30806.11\n",
            "--------------------------------------------------------------------------------\n",
            "A random forest ensemble had an RMSE of: 30680.90\n",
            "--------------------------------------------------------------------------------\n",
            "A gradient-boosted tree ensemble had an RMSE of: 29763.33\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 6. Compare the different models\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Initialize the evaluator for RMSE\n",
        "rmse_evaluator = RegressionEvaluator(labelCol=\"WAGE_RATE_OF_PAY_FROM\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Evaluate the models using RMSE\n",
        "dtr_rmse = rmse_evaluator.evaluate(dtr_predictions)\n",
        "rfr_rmse = rmse_evaluator.evaluate(rfr_predictions)\n",
        "gbtr_rmse = rmse_evaluator.evaluate(gbtr_predictions)\n",
        "\n",
        "print(\"Here are the results!\")\n",
        "print('-'*80)\n",
        "print('A single decision tree had an RMSE of: {0:2.2f}'.format(dtr_rmse))\n",
        "print('-'*80)\n",
        "print('A random forest ensemble had an RMSE of: {0:2.2f}'.format(rfr_rmse))\n",
        "print('-'*80)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse))\n",
        "print('-'*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UUqUCLEkGWCl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bd979243-62fc-42f5-a594-500f64a75a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:31:42] Predicting with Decision Tree Regressor...\n",
            "[2025-04-12 19:31:42] Predicting with Random Forest Regressor...\n",
            "[2025-04-12 19:31:42] Predicting with Gradient-Boosted Tree Regressor...\n",
            "A single decision tree had an RMSE of: 30860.31\n",
            "A random forest ensemble had an RMSE of: 30739.24\n",
            "A gradient-boosted tree ensemble had an RMSE of: 29803.27\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the models using RMSE\n",
        "print_progress(\"Predicting with Decision Tree Regressor...\")\n",
        "dtr_predictions_train = dtr_model.transform(train_data)\n",
        "\n",
        "print_progress(\"Predicting with Random Forest Regressor...\")\n",
        "rfr_predictions_train = rfr_model.transform(train_data)\n",
        "\n",
        "print_progress(\"Predicting with Gradient-Boosted Tree Regressor...\")\n",
        "gbtr_predictions_train = gbtr_model.transform(train_data)\n",
        "\n",
        "dtr_rmse_train = rmse_evaluator.evaluate(dtr_predictions_train)\n",
        "print('A single decision tree had an RMSE of: {0:2.2f}'.format(dtr_rmse_train))\n",
        "\n",
        "rfr_rmse_train = rmse_evaluator.evaluate(rfr_predictions_train)\n",
        "print('A random forest ensemble had an RMSE of: {0:2.2f}'.format(rfr_rmse_train))\n",
        "\n",
        "gbtr_rmse_train = rmse_evaluator.evaluate(gbtr_predictions_train)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "QQRyEozCGWCl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "75f63885-1c1d-4bfb-c09b-4a0709fdcc9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single decision tree had an r2 of: 0.22\n",
            "A random forest ensemble had an r2 of: 0.23\n",
            "A gradient-boosted tree ensemble had an RMSE of: 0.28\n",
            "A single decision tree had an r2 of: 0.22\n",
            "A random forest ensemble had an r2 of: 0.23\n",
            "A gradient-boosted tree ensemble had an RMSE of: 0.28\n"
          ]
        }
      ],
      "source": [
        "# r2\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"WAGE_RATE_OF_PAY_FROM\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "# Evaluate the models using r2_evaluator (test data)\n",
        "dtr_rmse_r2 = r2_evaluator.evaluate(dtr_predictions)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_r2))\n",
        "\n",
        "rfr_rmse_r2 = r2_evaluator.evaluate(rfr_predictions)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_r2))\n",
        "\n",
        "gbtr_rmse_r2 = r2_evaluator.evaluate(gbtr_predictions)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_r2))\n",
        "\n",
        "# Evaluate the models using r2_evaluator (train data)\n",
        "dtr_rmse_r2_train = r2_evaluator.evaluate(dtr_predictions_train)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_r2_train))\n",
        "\n",
        "rfr_rmse_r2_train = r2_evaluator.evaluate(rfr_predictions_train)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_r2_train))\n",
        "\n",
        "gbtr_rmse_r2_train = r2_evaluator.evaluate(gbtr_predictions_train)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_r2_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "46Oj72e_GWCl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "120f652f-601c-4c37-8dfa-8210e2c02eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single decision tree had an r2 of: 24568.15\n",
            "A random forest ensemble had an r2 of: 24579.61\n",
            "A gradient-boosted tree ensemble had an RMSE of: 23512.30\n",
            "A single decision tree had an r2 of: 24599.37\n",
            "A random forest ensemble had an r2 of: 24613.53\n",
            "A gradient-boosted tree ensemble had an RMSE of: 23537.02\n"
          ]
        }
      ],
      "source": [
        "# MAE\n",
        "mae_evaluator = RegressionEvaluator(labelCol=\"WAGE_RATE_OF_PAY_FROM\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "\n",
        "# Evaluate the models using r2_evaluator (test data)\n",
        "dtr_rmse_mae = mae_evaluator.evaluate(dtr_predictions)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_mae))\n",
        "\n",
        "rfr_rmse_mae = mae_evaluator.evaluate(rfr_predictions)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_mae))\n",
        "\n",
        "gbtr_rmse_mae = mae_evaluator.evaluate(gbtr_predictions)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_mae))\n",
        "\n",
        "# Evaluate the models using r2_evaluator (train data)\n",
        "dtr_rmse_train_mae = mae_evaluator.evaluate(dtr_predictions_train)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_train_mae))\n",
        "\n",
        "rfr_rmse_train_mae = mae_evaluator.evaluate(rfr_predictions_train)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_train_mae))\n",
        "\n",
        "gbtr_rmse_train_mae = mae_evaluator.evaluate(gbtr_predictions_train)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_train_mae))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX03u7xrGWCl"
      },
      "source": [
        "### Interpretation:\n",
        "\n",
        "- Decision Tree Regressor\n",
        "\n",
        "The difference between training and testing metrics is small, indicating that the Decision Tree model is not overfitting.\n",
        "Both R² values are quite low (0.22), indicating that the model is not capturing much of the variance, suggesting underfitting.\n",
        "\n",
        "Similar RMSE and MAE values in both training and testing imply that the model is consistently performing poorly on both datasets.\n",
        "\n",
        "- Random Forest Regressor\n",
        "\n",
        "The training and testing metrics are very similar, indicating that the Random Forest model generalizes well.\n",
        "\n",
        "The low R² (0.22) shows that the model is not explaining much variance, indicating potential underfitting.\n",
        "\n",
        "Despite being more robust than a single Decision Tree, the model may not be complex enough to capture patterns in the data.\n",
        "\n",
        "- Gradient-boosted Tree Regressor\n",
        "\n",
        "The Gradient-Boosted model has the lowest RMSE and MAE among the three models, indicating the best performance.\n",
        "\n",
        "The difference between training and testing metrics is still small, which means the model is not overfitting.\n",
        "\n",
        "The slightly higher R² value (0.26) indicates a marginal improvement over the other models, but it still struggles to capture variance, indicating some level of underfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4NIMR3g-F4t"
      },
      "source": [
        "## 2. Retrain (Use detrended salary, do regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hg2H3B5o-F4t"
      },
      "outputs": [],
      "source": [
        "# column_names = df_detrended.columns\n",
        "# print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tofbcfXY-F4u",
        "outputId": "7525c026-9f87-45f3-b07e-6a32e40d831d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+-----------------+\n",
            "|           Job_Group|Job_Group_Index|Job_Group_Encoded|\n",
            "+--------------------+---------------+-----------------+\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|                Data|            2.0|   (15,[2],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|Production, Const...|            3.0|   (15,[3],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "|    Computer Science|            0.0|   (15,[0],[1.0])|\n",
            "+--------------------+---------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Encoding EMPLOYER_STATE...\n",
            "Encoding WORKSITE_STATE...\n",
            "+----------------------------------------+-----------------+--------------+-------------------------+--------------+-------------------------+\n",
            "|Job_Group                               |Job_Group_Encoded|EMPLOYER_STATE|Employer_Location_Encoded|WORKSITE_STATE|Worksite_Location_Encoded|\n",
            "+----------------------------------------+-----------------+--------------+-------------------------+--------------+-------------------------+\n",
            "|Computer Science                        |(15,[0],[1.0])   |NJ            |(56,[2],[1.0])           |PA            |(54,[9],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |NY            |(56,[4],[1.0])           |NJ            |(54,[4],[1.0])           |\n",
            "|Data                                    |(15,[2],[1.0])   |NJ            |(56,[2],[1.0])           |NJ            |(54,[4],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |WA            |(56,[3],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |VA            |(56,[9],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |FL            |(54,[10],[1.0])          |\n",
            "|Computer Science                        |(15,[0],[1.0])   |WA            |(56,[3],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Production, Construction,& Manufacturing|(15,[3],[1.0])   |CA            |(56,[0],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |MO            |(56,[17],[1.0])          |MO            |(54,[20],[1.0])          |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |OR            |(54,[21],[1.0])          |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |WA            |(54,[3],[1.0])           |\n",
            "|Business & Accounting                   |(15,[1],[1.0])   |NJ            |(56,[2],[1.0])           |WA            |(54,[3],[1.0])           |\n",
            "|Business & Accounting                   |(15,[1],[1.0])   |NJ            |(56,[2],[1.0])           |TX            |(54,[1],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |TX            |(54,[1],[1.0])           |\n",
            "|Business & Accounting                   |(15,[1],[1.0])   |NY            |(56,[4],[1.0])           |FL            |(54,[10],[1.0])          |\n",
            "|Production, Construction,& Manufacturing|(15,[3],[1.0])   |TN            |(56,[16],[1.0])          |IN            |(54,[22],[1.0])          |\n",
            "|Computer Science                        |(15,[0],[1.0])   |CA            |(56,[0],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Data                                    |(15,[2],[1.0])   |CA            |(56,[0],[1.0])           |CA            |(54,[0],[1.0])           |\n",
            "|Computer Science                        |(15,[0],[1.0])   |WA            |(56,[3],[1.0])           |WA            |(54,[3],[1.0])           |\n",
            "|Healthcare                              |(15,[4],[1.0])   |NY            |(56,[4],[1.0])           |NY            |(54,[2],[1.0])           |\n",
            "+----------------------------------------+-----------------+--------------+-------------------------+--------------+-------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. string index and one-hot encode each nominal feature\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "# 1. Encode Job_Group\n",
        "job_indexer = StringIndexer(inputCol=\"Job_Group\", outputCol=\"Job_Group_Index\")\n",
        "df_indexed_job = job_indexer.fit(df_detrended).transform(df_detrended)\n",
        "\n",
        "job_encoder = OneHotEncoder(inputCol=\"Job_Group_Index\", outputCol=\"Job_Group_Encoded\")\n",
        "df_encoded_job = job_encoder.fit(df_indexed_job).transform(df_indexed_job)\n",
        "\n",
        "df_encoded_job.select(\"Job_Group\", \"Job_Group_Index\", \"Job_Group_Encoded\").show(10)\n",
        "\n",
        "# 2. Encode EMPLOYER_STATE\n",
        "print(\"Encoding EMPLOYER_STATE...\")\n",
        "employer_indexer = StringIndexer(inputCol=\"EMPLOYER_STATE\", outputCol=\"Employer_Location_Index\")\n",
        "df_indexed_employer = employer_indexer.fit(df_encoded_job).transform(df_encoded_job)\n",
        "\n",
        "employer_encoder = OneHotEncoder(inputCol=\"Employer_Location_Index\", outputCol=\"Employer_Location_Encoded\")\n",
        "df_encoded_employer = employer_encoder.fit(df_indexed_employer).transform(df_indexed_employer)\n",
        "\n",
        "# 3. Encode WORKSITE_STATE\n",
        "print(\"Encoding WORKSITE_STATE...\")\n",
        "worksite_indexer = StringIndexer(inputCol=\"WORKSITE_STATE\", outputCol=\"Worksite_Location_Index\")\n",
        "df_indexed_worksite = worksite_indexer.fit(df_encoded_employer).transform(df_encoded_employer)\n",
        "\n",
        "worksite_encoder = OneHotEncoder(inputCol=\"Worksite_Location_Index\", outputCol=\"Worksite_Location_Encoded\")\n",
        "final_encoded_df = worksite_encoder.fit(df_indexed_worksite).transform(df_indexed_worksite)\n",
        "\n",
        "# Show the result\n",
        "final_encoded_df.select(\n",
        "    \"Job_Group\", \"Job_Group_Encoded\",\n",
        "    \"EMPLOYER_STATE\", \"Employer_Location_Encoded\",\n",
        "    \"WORKSITE_STATE\", \"Worksite_Location_Encoded\"\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Yq5nvYBn-F4u"
      },
      "outputs": [],
      "source": [
        "# 2. bundle all the 4 input features together\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Bundle the encoded features into a single vector column\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Job_Group_Encoded\", \"Employer_Location_Encoded\", \"Worksite_Location_Encoded\", \"yrs_of_experience\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Apply the assembler to the final encoded DataFrame\n",
        "vectorized_df = assembler.transform(final_encoded_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PZyyrF6b-F4u",
        "outputId": "8d5f23ed-f4e6-457f-c25a-f5127a71ea5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+\n",
            "|            features|    WAGE_DETRENDED|\n",
            "+--------------------+------------------+\n",
            "|(126,[0,17,80,125...|119649.47147181997|\n",
            "|(126,[0,19,75,125...|116950.61121305711|\n",
            "|(126,[2,17,75,125...| 96262.31232656725|\n",
            "|(126,[0,18,71,125...| 174957.2147546472|\n",
            "|(126,[0,24,71,125...|157433.51509449995|\n",
            "+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. choose the data\n",
        "final_data_detrended = vectorized_df.select(\"features\",'WAGE_DETRENDED')\n",
        "final_data_detrended.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jPQukBKL-F4u"
      },
      "outputs": [],
      "source": [
        "# 4. split the train and test data\n",
        "train_data,test_data = final_data_detrended.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mOb5DUI4-F4u",
        "outputId": "9d519def-f74a-4844-887d-8415843e64d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:41:38] Starting training: Decision Tree Regressor...\n",
            "[2025-04-12 19:41:38] Fitting the Decision Tree model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Decision Tree: 100%|██████████| 1/1 [01:59<00:00, 119.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:43:37] Training completed: Decision Tree Regressor.\n",
            "[2025-04-12 19:43:37] Predicting with Decision Tree Regressor...\n",
            "[2025-04-12 19:43:37] Prediction completed: Decision Tree Regressor.\n",
            "[2025-04-12 19:43:37] Starting training: Random Forest Regressor...\n",
            "[2025-04-12 19:43:37] Fitting the Random Forest model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Random Forest: 100%|██████████| 1/1 [02:17<00:00, 137.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:45:55] Training completed: Random Forest Regressor.\n",
            "[2025-04-12 19:45:55] Predicting with Random Forest Regressor...\n",
            "[2025-04-12 19:45:55] Prediction completed: Random Forest Regressor.\n",
            "[2025-04-12 19:45:55] Starting training: Gradient-Boosted Tree Regressor...\n",
            "[2025-04-12 19:45:55] Fitting the Gradient-Boosted Tree model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Gradient-Boosted Tree: 100%|██████████| 1/1 [09:00<00:00, 540.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:54:56] Training completed: Gradient-Boosted Tree Regressor.\n",
            "[2025-04-12 19:54:56] Predicting with Gradient-Boosted Tree Regressor...\n",
            "[2025-04-12 19:54:56] Prediction completed: Gradient-Boosted Tree Regressor.\n",
            "[2025-04-12 19:54:56] All models trained and predictions made successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor, RandomForestRegressor\n",
        "from pyspark.sql import SparkSession\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# 5. train the model\n",
        "def print_progress(message):\n",
        "    \"\"\"Helper function to print progress with a timestamp.\"\"\"\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Step 1: Train Decision Tree Regressor\n",
        "print_progress(\"Starting training: Decision Tree Regressor...\")\n",
        "dtr = DecisionTreeRegressor(labelCol='WAGE_DETRENDED', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Decision Tree model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Decision Tree\"):\n",
        "    dtr_model = dtr.fit(train_data)\n",
        "print_progress(\"Training completed: Decision Tree Regressor.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Decision Tree Regressor...\")\n",
        "dtr_predictions = dtr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Decision Tree Regressor.\")\n",
        "\n",
        "# Step 2: Train Random Forest Regressor\n",
        "print_progress(\"Starting training: Random Forest Regressor...\")\n",
        "rfr = RandomForestRegressor(labelCol='WAGE_DETRENDED', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Random Forest model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Random Forest\"):\n",
        "    rfr_model = rfr.fit(train_data)\n",
        "print_progress(\"Training completed: Random Forest Regressor.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Random Forest Regressor...\")\n",
        "rfr_predictions = rfr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Random Forest Regressor.\")\n",
        "\n",
        "# Step 3: Train Gradient-Boosted Tree Regressor\n",
        "print_progress(\"Starting training: Gradient-Boosted Tree Regressor...\")\n",
        "gbtr = GBTRegressor(labelCol='WAGE_DETRENDED', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Gradient-Boosted Tree model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Gradient-Boosted Tree\"):\n",
        "    gbtr_model = gbtr.fit(train_data)\n",
        "print_progress(\"Training completed: Gradient-Boosted Tree Regressor.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Gradient-Boosted Tree Regressor...\")\n",
        "gbtr_predictions = gbtr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Gradient-Boosted Tree Regressor.\")\n",
        "\n",
        "print_progress(\"All models trained and predictions made successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IvrRoLBg-F4u",
        "outputId": "673df08a-8b13-4b09-b605-adb4db9aec4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the results!\n",
            "--------------------------------------------------------------------------------\n",
            "A single decision tree had an RMSE of: 28604.10\n",
            "--------------------------------------------------------------------------------\n",
            "A random forest ensemble had an RMSE of: 28468.69\n",
            "--------------------------------------------------------------------------------\n",
            "A gradient-boosted tree ensemble had an RMSE of: 27611.71\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 6. Compare the different models\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Initialize the evaluator for RMSE\n",
        "rmse_evaluator = RegressionEvaluator(labelCol=\"WAGE_DETRENDED\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Evaluate the models using RMSE\n",
        "dtr_rmse = rmse_evaluator.evaluate(dtr_predictions)\n",
        "rfr_rmse = rmse_evaluator.evaluate(rfr_predictions)\n",
        "gbtr_rmse = rmse_evaluator.evaluate(gbtr_predictions)\n",
        "\n",
        "print(\"Here are the results!\")\n",
        "print('-'*80)\n",
        "print('A single decision tree had an RMSE of: {0:2.2f}'.format(dtr_rmse))\n",
        "print('-'*80)\n",
        "print('A random forest ensemble had an RMSE of: {0:2.2f}'.format(rfr_rmse))\n",
        "print('-'*80)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse))\n",
        "print('-'*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vyhr_PfM-F4u",
        "outputId": "e6d52d71-936c-4fec-f3bd-3e176f2d29d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 19:57:50] Predicting with Decision Tree Regressor...\n",
            "[2025-04-12 19:57:51] Predicting with Random Forest Regressor...\n",
            "[2025-04-12 19:57:51] Predicting with Gradient-Boosted Tree Regressor...\n",
            "A single decision tree had an RMSE of: 28571.50\n",
            "A random forest ensemble had an RMSE of: 28434.47\n",
            "A gradient-boosted tree ensemble had an RMSE of: 27576.20\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the models using RMSE-continue\n",
        "print_progress(\"Predicting with Decision Tree Regressor...\")\n",
        "dtr_predictions_train = dtr_model.transform(train_data)\n",
        "\n",
        "print_progress(\"Predicting with Random Forest Regressor...\")\n",
        "rfr_predictions_train = rfr_model.transform(train_data)\n",
        "\n",
        "print_progress(\"Predicting with Gradient-Boosted Tree Regressor...\")\n",
        "gbtr_predictions_train = gbtr_model.transform(train_data)\n",
        "\n",
        "dtr_rmse_train = rmse_evaluator.evaluate(dtr_predictions_train)\n",
        "print('A single decision tree had an RMSE of: {0:2.2f}'.format(dtr_rmse_train))\n",
        "\n",
        "rfr_rmse_train = rmse_evaluator.evaluate(rfr_predictions_train)\n",
        "print('A random forest ensemble had an RMSE of: {0:2.2f}'.format(rfr_rmse_train))\n",
        "\n",
        "gbtr_rmse_train = rmse_evaluator.evaluate(gbtr_predictions_train)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "P-j0waMj-F4u",
        "outputId": "b07c01e5-9edb-4538-87ed-b106a47aea27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single decision tree had an r2 of: 0.22\n",
            "A random forest ensemble had an r2 of: 0.23\n",
            "A gradient-boosted tree ensemble had an RMSE of: 0.27\n",
            "A single decision tree had an r2 of: 0.22\n",
            "A random forest ensemble had an r2 of: 0.23\n",
            "A gradient-boosted tree ensemble had an RMSE of: 0.28\n"
          ]
        }
      ],
      "source": [
        "# r2\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"WAGE_DETRENDED\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "# Evaluate the models using r2_evaluator (test data)\n",
        "dtr_rmse_r2 = r2_evaluator.evaluate(dtr_predictions)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_r2))\n",
        "\n",
        "rfr_rmse_r2 = r2_evaluator.evaluate(rfr_predictions)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_r2))\n",
        "\n",
        "gbtr_rmse_r2 = r2_evaluator.evaluate(gbtr_predictions)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_r2))\n",
        "\n",
        "# Evaluate the models using r2_evaluator (train data)\n",
        "dtr_rmse_r2_train = r2_evaluator.evaluate(dtr_predictions_train)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_r2_train))\n",
        "\n",
        "rfr_rmse_r2_train = r2_evaluator.evaluate(rfr_predictions_train)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_r2_train))\n",
        "\n",
        "gbtr_rmse_r2_train = r2_evaluator.evaluate(gbtr_predictions_train)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_r2_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1o8QQYbT-F4v",
        "outputId": "f62a7385-50c6-4332-816e-4e3134e68175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single decision tree had an r2 of: 22677.41\n",
            "A random forest ensemble had an r2 of: 22649.50\n",
            "A gradient-boosted tree ensemble had an RMSE of: 21712.26\n",
            "A single decision tree had an r2 of: 22656.87\n",
            "A random forest ensemble had an r2 of: 22623.68\n",
            "A gradient-boosted tree ensemble had an RMSE of: 21692.12\n"
          ]
        }
      ],
      "source": [
        "# MAE\n",
        "mae_evaluator = RegressionEvaluator(labelCol=\"WAGE_DETRENDED\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "\n",
        "# Evaluate the models using r2_evaluator (test data)\n",
        "dtr_rmse_mae = mae_evaluator.evaluate(dtr_predictions)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_mae))\n",
        "\n",
        "rfr_rmse_mae = mae_evaluator.evaluate(rfr_predictions)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_mae))\n",
        "\n",
        "gbtr_rmse_mae = mae_evaluator.evaluate(gbtr_predictions)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_mae))\n",
        "\n",
        "# Evaluate the models using r2_evaluator (train data)\n",
        "dtr_rmse_train_mae = mae_evaluator.evaluate(dtr_predictions_train)\n",
        "print('A single decision tree had an r2 of: {0:2.2f}'.format(dtr_rmse_train_mae))\n",
        "\n",
        "rfr_rmse_train_mae = mae_evaluator.evaluate(rfr_predictions_train)\n",
        "print('A random forest ensemble had an r2 of: {0:2.2f}'.format(rfr_rmse_train_mae))\n",
        "\n",
        "gbtr_rmse_train_mae = mae_evaluator.evaluate(gbtr_predictions_train)\n",
        "print('A gradient-boosted tree ensemble had an RMSE of: {0:2.2f}'.format(gbtr_rmse_train_mae))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_detrended.select('yrs_of_experience').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF1s3o_gq7eu",
        "outputId": "eee63bb8-fd27-4b5f-a4ad-c159d84f3289"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|yrs_of_experience|\n",
            "+-----------------+\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|             2.99|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "|              3.0|\n",
            "+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g782oXzJ-F4v",
        "outputId": "c50182a8-5c91-455c-8b32-a07f6ffd754b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 20:29:21] Starting Decision Tree Regressor training with cross-validation (maxDepth = 5, 10)...\n",
            "[2025-04-12 20:39:48] Training completed with cross-validation.\n",
            "[2025-04-12 20:40:46] Best Decision Tree Regressor Parameter: maxDepth = 10\n",
            "[2025-04-12 20:40:46] Test RMSE of best model: 27806.1708\n",
            "[2025-04-12 20:41:44] Training RMSE of best model: 27767.1092\n"
          ]
        }
      ],
      "source": [
        "# 7.1 Finetune decision tree (maxDepth)\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "import time\n",
        "\n",
        "# Print progress function\n",
        "def print_progress(message):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "dtr = DecisionTreeRegressor(labelCol='WAGE_DETRENDED', featuresCol='features')\n",
        "\n",
        "# Build parameter grid with only two maxDepth values\n",
        "paramGrid_dtr = (ParamGridBuilder()\n",
        "                 .addGrid(dtr.maxDepth, [5, 10])\n",
        "                 .build())\n",
        "\n",
        "# Regression evaluator using RMSE\n",
        "reg_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"WAGE_DETRENDED\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "\n",
        "# Set up cross-validation\n",
        "cv_dtr = CrossValidator(\n",
        "    estimator=dtr,\n",
        "    estimatorParamMaps=paramGrid_dtr,\n",
        "    evaluator=reg_evaluator,\n",
        "    numFolds=3,\n",
        "    parallelism=2\n",
        ")\n",
        "\n",
        "# Train with cross-validation\n",
        "print_progress(\"Starting Decision Tree Regressor training with cross-validation (maxDepth = 5, 10)...\")\n",
        "cv_dtr_model = cv_dtr.fit(train_data)\n",
        "print_progress(\"Training completed with cross-validation.\")\n",
        "\n",
        "# Get best model and its parameters\n",
        "best_dtr_model = cv_dtr_model.bestModel\n",
        "best_max_depth = best_dtr_model.getOrDefault(\"maxDepth\")\n",
        "\n",
        "# Make predictions and evaluate RMSE on test set\n",
        "dtr_predictions = best_dtr_model.transform(test_data)\n",
        "rmse = reg_evaluator.evaluate(dtr_predictions)\n",
        "\n",
        "# Print results\n",
        "print_progress(f\"Best Decision Tree Regressor Parameter: maxDepth = {best_max_depth}\")\n",
        "print_progress(f\"Test RMSE of best model: {rmse:.4f}\")\n",
        "\n",
        "# Make predictions on the training dataset using the best model\n",
        "dtr_train_predictions = best_dtr_model.transform(train_data)\n",
        "\n",
        "# Evaluate RMSE on the training dataset\n",
        "train_rmse = reg_evaluator.evaluate(dtr_train_predictions)\n",
        "\n",
        "# Print training RMSE\n",
        "print_progress(f\"Training RMSE of best model: {train_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2 Finetune random forest\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import time\n",
        "\n",
        "# Print progress function\n",
        "def print_progress(message):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "rfr = RandomForestRegressor(labelCol='WAGE_DETRENDED', featuresCol='features')\n",
        "\n",
        "# Build parameter grid (e.g., maxDepth and numTrees)\n",
        "paramGrid_rfr = (ParamGridBuilder()\n",
        "                 .addGrid(rfr.maxDepth, [5, 10])\n",
        "                 .addGrid(rfr.numTrees, [20, 50])\n",
        "                 .build())\n",
        "\n",
        "# Regression evaluator using RMSE\n",
        "reg_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"WAGE_DETRENDED\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "\n",
        "# Set up cross-validation\n",
        "cv_rfr = CrossValidator(\n",
        "    estimator=rfr,\n",
        "    estimatorParamMaps=paramGrid_rfr,\n",
        "    evaluator=reg_evaluator,\n",
        "    numFolds=3,\n",
        "    parallelism=2\n",
        ")\n",
        "\n",
        "# Train with cross-validation\n",
        "print_progress(\"Starting Random Forest Regressor training with cross-validation...\")\n",
        "cv_rfr_model = cv_rfr.fit(train_data)\n",
        "print_progress(\"Training completed with cross-validation.\")\n",
        "\n",
        "# Get best model and parameters\n",
        "best_rfr_model = cv_rfr_model.bestModel\n",
        "best_max_depth = best_rfr_model.getOrDefault(\"maxDepth\")\n",
        "best_num_trees = best_rfr_model.getOrDefault(\"numTrees\")\n",
        "\n",
        "# Evaluate on test data\n",
        "rfr_predictions = best_rfr_model.transform(test_data)\n",
        "test_rmse = reg_evaluator.evaluate(rfr_predictions)\n",
        "print_progress(f\"Best Random Forest Parameters: maxDepth = {best_max_depth}, numTrees = {best_num_trees}\")\n",
        "print_progress(f\"Test RMSE of best Random Forest model: {test_rmse:.4f}\")\n",
        "\n",
        "# Evaluate on training data\n",
        "rfr_train_predictions = best_rfr_model.transform(train_data)\n",
        "train_rmse = reg_evaluator.evaluate(rfr_train_predictions)\n",
        "print_progress(f\"Training RMSE of best Random Forest model: {train_rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1XFcuTvxJ0Y",
        "outputId": "d5d8f42d-b478-4042-d3be-144fbbada7ed"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 20:53:32] Starting Random Forest Regressor training with cross-validation...\n",
            "[2025-04-12 21:17:36] Training completed with cross-validation.\n",
            "[2025-04-12 21:18:38] Best Random Forest Parameters: maxDepth = 10, numTrees = 50\n",
            "[2025-04-12 21:18:38] Test RMSE of best Random Forest model: 27717.0163\n",
            "[2025-04-12 21:19:45] Training RMSE of best Random Forest model: 27677.8554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.3 Finetune Gradient-Boosted Tree Regressor\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import time\n",
        "\n",
        "# Print progress function\n",
        "def print_progress(message):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Initialize GBT Regressor\n",
        "gbt = GBTRegressor(labelCol='WAGE_DETRENDED', featuresCol='features')\n",
        "\n",
        "# Build parameter grid (e.g., maxDepth and maxIter)\n",
        "paramGrid_gbt = (ParamGridBuilder()\n",
        "                 .addGrid(gbt.maxDepth, [5, 10])\n",
        "                 .addGrid(gbt.maxIter, [20, 50])  # maxIter = number of boosting rounds\n",
        "                 .build())\n",
        "\n",
        "# Regression evaluator using RMSE\n",
        "reg_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"WAGE_DETRENDED\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "\n",
        "# Set up cross-validation\n",
        "cv_gbt = CrossValidator(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=paramGrid_gbt,\n",
        "    evaluator=reg_evaluator,\n",
        "    numFolds=3,\n",
        "    parallelism=2\n",
        ")\n",
        "\n",
        "# Train with cross-validation\n",
        "print_progress(\"Starting GBT Regressor training with cross-validation...\")\n",
        "cv_gbt_model = cv_gbt.fit(train_data)\n",
        "print_progress(\"Training completed with cross-validation.\")\n",
        "\n",
        "# Get best model and parameters\n",
        "best_gbt_model = cv_gbt_model.bestModel\n",
        "best_max_depth = best_gbt_model.getOrDefault(\"maxDepth\")\n",
        "best_max_iter = best_gbt_model.getOrDefault(\"maxIter\")\n",
        "\n",
        "# Evaluate on test data\n",
        "gbt_predictions = best_gbt_model.transform(test_data)\n",
        "test_rmse = reg_evaluator.evaluate(gbt_predictions)\n",
        "print_progress(f\"Best GBT Parameters: maxDepth = {best_max_depth}, maxIter = {best_max_iter}\")\n",
        "print_progress(f\"Test RMSE of best GBT model: {test_rmse:.4f}\")\n",
        "\n",
        "# Evaluate on training data\n",
        "gbt_train_predictions = best_gbt_model.transform(train_data)\n",
        "train_rmse = reg_evaluator.evaluate(gbt_train_predictions)\n",
        "print_progress(f\"Training RMSE of best GBT model: {train_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5XszIks38HT",
        "outputId": "e6f875d1-7aba-42ae-a1aa-1b2a7282e3bc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-12 21:20:39] Starting GBT Regressor training with cross-validation...\n",
            "[2025-04-13 01:04:55] Training completed with cross-validation.\n",
            "[2025-04-13 01:05:59] Best GBT Parameters: maxDepth = 10, maxIter = 50\n",
            "[2025-04-13 01:05:59] Test RMSE of best GBT model: 27041.1945\n",
            "[2025-04-13 01:07:10] Training RMSE of best GBT model: 26960.2027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfFuBCtDGWCl"
      },
      "source": [
        "## 3. Retrain (Classification)\n",
        "We train the tree model again after we change output from continous variable (wage) to catogory (wage bucket)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "FO7Zd4j1GWCl",
        "outputId": "cbc88659-4806-4bc3-acd5-ebf1bec28f53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Job_Group', 'YEAR', 'CASE_NUMBER', 'CASE_STATUS', 'RECEIVED_DATE', 'DECISION_DATE', 'VISA_CLASS', 'JOB_TITLE', 'SOC_CODE', 'SOC_TITLE', 'FULL_TIME_POSITION', 'BEGIN_DATE', 'END_DATE', 'TOTAL_WORKER_POSITIONS', 'NEW_EMPLOYMENT', 'CONTINUED_EMPLOYMENT', 'CHANGE_PREVIOUS_EMPLOYMENT', 'NEW_CONCURRENT_EMPLOYMENT', 'CHANGE_EMPLOYER', 'AMENDED_PETITION', 'EMPLOYER_NAME', 'EMPLOYER_ADDRESS1', 'EMPLOYER_CITY', 'EMPLOYER_STATE', 'EMPLOYER_POSTAL_CODE', 'EMPLOYER_COUNTRY', 'EMPLOYER_PHONE', 'NAICS_CODE', 'EMPLOYER_POC_LAST_NAME', 'EMPLOYER_POC_FIRST_NAME', 'EMPLOYER_POC_JOB_TITLE', 'EMPLOYER_POC_ADDRESS1', 'EMPLOYER_POC_CITY', 'EMPLOYER_POC_STATE', 'EMPLOYER_POC_POSTAL_CODE', 'EMPLOYER_POC_COUNTRY', 'EMPLOYER_POC_PHONE', 'EMPLOYER_POC_EMAIL', 'AGENT_REPRESENTING_EMPLOYER', 'AGENT_ATTORNEY_LAST_NAME', 'AGENT_ATTORNEY_FIRST_NAME', 'AGENT_ATTORNEY_ADDRESS1', 'AGENT_ATTORNEY_ADDRESS2', 'AGENT_ATTORNEY_CITY', 'AGENT_ATTORNEY_STATE', 'AGENT_ATTORNEY_POSTAL_CODE', 'AGENT_ATTORNEY_COUNTRY', 'AGENT_ATTORNEY_PHONE', 'AGENT_ATTORNEY_EMAIL_ADDRESS', 'LAWFIRM_NAME_BUSINESS_NAME', 'STATE_OF_HIGHEST_COURT', 'NAME_OF_HIGHEST_STATE_COURT', 'WORKSITE_WORKERS', 'SECONDARY_ENTITY', 'WORKSITE_ADDRESS1', 'WORKSITE_CITY', 'WORKSITE_COUNTY', 'WORKSITE_STATE', 'WORKSITE_POSTAL_CODE', 'WAGE_RATE_OF_PAY_FROM', 'WAGE_UNIT_OF_PAY', 'PREVAILING_WAGE', 'PW_UNIT_OF_PAY', 'PW_WAGE_LEVEL', 'PW_OES_YEAR', 'TOTAL_WORKSITE_LOCATIONS', 'AGREE_TO_LC_STATEMENT', 'H_1B_DEPENDENT', 'WILLFUL_VIOLATOR', 'PUBLIC_DISCLOSURE', 'PREPARER_BUSINESS_NAME', 'PREPARER_EMAIL', 'BEGIN_DATE_PARSED', 'END_DATE_PARSED', 'yrs_of_experience', 'multiplier', 'WAGE_DETRENDED', 'Wage Bucket', 'wage_bucket']\n"
          ]
        }
      ],
      "source": [
        "column_names = df_buckets.columns\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0Xb-q0m1GWCm",
        "outputId": "35f0744d-9982-4b65-dbb6-d47e46f1c108",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        }
      ],
      "source": [
        "# 1. string index and one-hot encode each nominal feature (same as before)\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# 1. Job Group\n",
        "indexer = StringIndexer(inputCol=\"Job_Group\", outputCol=\"Job_Group_Index\")\n",
        "indexed_df = indexer.fit(df_buckets).transform(df_buckets)\n",
        "\n",
        "encoder = OneHotEncoder(inputCols=[\"Job_Group_Index\"], outputCols=[\"Job_Group_Encoded\"])\n",
        "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
        "\n",
        "# 2. Employer Location\n",
        "employer_indexer = StringIndexer(inputCol=\"EMPLOYER_STATE\", outputCol=\"Employer_Location_Index\")\n",
        "employer_indexed_df = employer_indexer.fit(encoded_df).transform(encoded_df)\n",
        "\n",
        "employer_encoder = OneHotEncoder(inputCols=[\"Employer_Location_Index\"], outputCols=[\"Employer_Location_Encoded\"])\n",
        "employer_encoded_df = employer_encoder.fit(employer_indexed_df).transform(employer_indexed_df)\n",
        "\n",
        "# 3. Worksite Location\n",
        "worksite_indexer = StringIndexer(inputCol=\"WORKSITE_STATE\", outputCol=\"Worksite_Location_Index\")\n",
        "worksite_indexed_df = worksite_indexer.fit(employer_encoded_df).transform(employer_encoded_df)\n",
        "\n",
        "worksite_encoder = OneHotEncoder(inputCols=[\"Worksite_Location_Index\"], outputCols=[\"Worksite_Location_Encoded\"])\n",
        "final_encoded_df = worksite_encoder.fit(worksite_indexed_df).transform(worksite_indexed_df)\n",
        "\n",
        "print(\"here\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OO6FUm5ZGWCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4baa5b8f-7881-4c7d-ea58-fd5a28bfaadf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        }
      ],
      "source": [
        "# 2. bundle all the 4 input features together\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Bundle the encoded features into a single vector column\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Job_Group_Encoded\", \"Employer_Location_Encoded\", \"Worksite_Location_Encoded\", \"yrs_of_experience\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Apply the assembler to the final encoded DataFrame\n",
        "vectorized_df = assembler.transform(final_encoded_df)\n",
        "print(\"here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NF44fqp4GWCm",
        "outputId": "7c99af34-b120-438d-a34d-f12895242d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+\n",
            "|     wage_bucket|wage_bucket_index|\n",
            "+----------------+-----------------+\n",
            "|(120000, 140000)|              3.0|\n",
            "|(120000, 140000)|              3.0|\n",
            "|(100000, 120000)|              1.0|\n",
            "|(180000, 200000)|              6.0|\n",
            "|(160000, 180000)|              5.0|\n",
            "|(120000, 140000)|              3.0|\n",
            "|(160000, 180000)|              5.0|\n",
            "|(120000, 140000)|              3.0|\n",
            "|(100000, 120000)|              1.0|\n",
            "|(100000, 120000)|              1.0|\n",
            "+----------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "here\n"
          ]
        }
      ],
      "source": [
        "# 3. string indexer output feature (we do not need to do one hot encoder)\n",
        "bucket_indexer = StringIndexer(inputCol=\"wage_bucket\", outputCol=\"wage_bucket_index\")\n",
        "vectorized_df = bucket_indexer.fit(vectorized_df).transform(vectorized_df)\n",
        "vectorized_df.select(\"wage_bucket\", \"wage_bucket_index\").show(10)\n",
        "print(\"here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "SU_lUxzEGWCm",
        "outputId": "c9f01f7f-7cea-4d64-f2bb-d4b86f5d6c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+\n",
            "|            features|wage_bucket_index|\n",
            "+--------------------+-----------------+\n",
            "|(126,[0,17,80,125...|              3.0|\n",
            "|(126,[0,19,75,125...|              3.0|\n",
            "|(126,[2,17,75,125...|              1.0|\n",
            "|(126,[0,18,71,125...|              6.0|\n",
            "|(126,[0,24,71,125...|              5.0|\n",
            "+--------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. choose the data\n",
        "final_data = vectorized_df.select(\"features\",'wage_bucket_index')\n",
        "final_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "89ShpdwXGWCm"
      },
      "outputs": [],
      "source": [
        "# 5. split the train and test data\n",
        "train_data,test_data = final_data.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "b_4IpXQuGWCn",
        "outputId": "338d0b12-799c-4d0e-9caf-b52a0f40b78a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-13 03:26:26] Starting training: Decision Tree Classifier...\n",
            "[2025-04-13 03:26:26] Fitting the Decision Tree model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Decision Tree: 100%|██████████| 1/1 [02:57<00:00, 177.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-13 03:29:23] Training completed: Decision Tree Classifier.\n",
            "[2025-04-13 03:29:23] Predicting with Decision Tree Classifier...\n",
            "[2025-04-13 03:29:24] Prediction completed: Decision Tree Classifier.\n",
            "[2025-04-13 03:29:24] Starting training: Random Forest Classifier...\n",
            "[2025-04-13 03:29:24] Fitting the Random Forest model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Random Forest: 100%|██████████| 1/1 [04:10<00:00, 250.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-13 03:33:34] Training completed: Random Forest Classifier.\n",
            "[2025-04-13 03:33:34] Predicting with Random Forest Classifier...\n",
            "[2025-04-13 03:33:35] Prediction completed: Random Forest Classifier.\n"
          ]
        }
      ],
      "source": [
        "# 6. train decision tree classifier and random forest classifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier,  RandomForestClassifier\n",
        "from pyspark.sql import SparkSession\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Helper function to print progress with a timestamp\n",
        "def print_progress(message):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Step 1: Train Decision Tree Classifier\n",
        "print_progress(\"Starting training: Decision Tree Classifier...\")\n",
        "dtr = DecisionTreeClassifier(labelCol='wage_bucket_index', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Decision Tree model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Decision Tree\"):\n",
        "    dtr_model = dtr.fit(train_data)\n",
        "print_progress(\"Training completed: Decision Tree Classifier.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Decision Tree Classifier...\")\n",
        "dtr_predictions = dtr_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Decision Tree Classifier.\")\n",
        "\n",
        "# Step 2: Train Random Forest Classifier\n",
        "print_progress(\"Starting training: Random Forest Classifier...\")\n",
        "rfc = RandomForestClassifier(labelCol='wage_bucket_index', featuresCol='features')\n",
        "\n",
        "# Progress bar for training\n",
        "print_progress(\"Fitting the Random Forest model...\")\n",
        "for _ in tqdm(range(1), desc=\"Training Random Forest\"):\n",
        "    rfc_model = rfc.fit(train_data)\n",
        "print_progress(\"Training completed: Random Forest Classifier.\")\n",
        "\n",
        "# Make predictions\n",
        "print_progress(\"Predicting with Random Forest Classifier...\")\n",
        "rfc_predictions = rfc_model.transform(test_data)\n",
        "print_progress(\"Prediction completed: Random Forest Classifier.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "A6mhm05eGWCn",
        "outputId": "aa762255-e4b9-463b-fb07-ae5a85977ff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier - Training Accuracy: 0.29\n",
            "Random Forest Classifier - Training Accuracy: 0.28\n"
          ]
        }
      ],
      "source": [
        "# 6. Compare the different models\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Initialize the evaluator for accuracy\n",
        "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"wage_bucket_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Evaluate the models using accuracy on training data\n",
        "dtr_train_acc = acc_evaluator.evaluate(dtr_model.transform(train_data))\n",
        "print('Decision Tree Classifier - Training Accuracy: {0:2.2f}'.format(dtr_train_acc))\n",
        "rfc_train_acc = acc_evaluator.evaluate(rfc_model.transform(train_data))\n",
        "print('Random Forest Classifier - Training Accuracy: {0:2.2f}'.format(rfc_train_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxoeAuud-F4w",
        "outputId": "ee8a5a85-b17c-4108-e290-ca981739036c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier - Testing Accuracy: 0.29\n",
            "Random Forest Classifier - Testing Accuracy: 0.28\n"
          ]
        }
      ],
      "source": [
        "# Initialize the evaluator for accuracy\n",
        "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"wage_bucket_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Evaluate the models using accuracy on testing data\n",
        "dtr_test_acc = acc_evaluator.evaluate(dtr_predictions)\n",
        "print('Decision Tree Classifier - Testing Accuracy: {0:2.2f}'.format(dtr_test_acc))\n",
        "rfc_test_acc = acc_evaluator.evaluate(rfc_predictions)\n",
        "print('Random Forest Classifier - Testing Accuracy: {0:2.2f}'.format(rfc_test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "zax-bVdEGWCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00746957-521b-4ab6-eaf9-0197539b03b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-13 03:44:23] Starting Decision Tree training with cross-validation...\n",
            "[2025-04-13 03:57:24] Training completed.\n",
            "[2025-04-13 03:57:24] Best maxDepth: 10\n",
            "[2025-04-13 03:58:24] Training Accuracy: 0.3119\n",
            "[2025-04-13 03:59:24] Test Accuracy: 0.3110\n",
            "[2025-04-13 03:59:24] Best Cross-Validation Accuracy: 0.3122\n"
          ]
        }
      ],
      "source": [
        "# 7.1 finetune the parameters- decision tree\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "import time\n",
        "\n",
        "# Utility function to print progress with timestamps\n",
        "def print_progress(message):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(labelCol='wage_bucket_index', featuresCol='features')\n",
        "\n",
        "# Create parameter grid for tuning\n",
        "paramGrid_dt = (ParamGridBuilder()\n",
        "                .addGrid(dt.maxDepth, [5, 10])\n",
        "                .build())\n",
        "\n",
        "# Set up evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"wage_bucket_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Set up cross-validation\n",
        "cv_dt = CrossValidator(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid_dt,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3,\n",
        "    parallelism=2\n",
        ")\n",
        "\n",
        "# Train the model with cross-validation\n",
        "print_progress(\"Starting Decision Tree training with cross-validation...\")\n",
        "dt_model = cv_dt.fit(train_data)\n",
        "print_progress(\"Training completed.\")\n",
        "\n",
        "# Extract the best model and its parameters\n",
        "best_model = dt_model.bestModel\n",
        "best_max_depth = best_model.getMaxDepth()\n",
        "print_progress(f\"Best maxDepth: {best_max_depth}\")\n",
        "\n",
        "# Evaluate on training data\n",
        "train_predictions = best_model.transform(train_data)\n",
        "train_accuracy = evaluator.evaluate(train_predictions)\n",
        "print_progress(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_predictions = best_model.transform(test_data)\n",
        "test_accuracy = evaluator.evaluate(test_predictions)\n",
        "print_progress(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Cross-validation best accuracy\n",
        "best_index = dt_model.avgMetrics.index(max(dt_model.avgMetrics))\n",
        "best_cv_accuracy = dt_model.avgMetrics[best_index]\n",
        "print_progress(f\"Best Cross-Validation Accuracy: {best_cv_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2 finetune the parameters- random forest\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import time\n",
        "\n",
        "# Function to print progress\n",
        "def print_progress(message):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol='wage_bucket_index', featuresCol='features')\n",
        "\n",
        "# Create parameter grid\n",
        "paramGrid_rf = (ParamGridBuilder()\n",
        "                .addGrid(rf.numTrees, [20, 50])\n",
        "                .addGrid(rf.maxDepth, [5, 10])\n",
        "                .build())\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"wage_bucket_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Set up cross-validation\n",
        "cv_rf = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid_rf,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3,\n",
        "    parallelism=2\n",
        ")\n",
        "\n",
        "# Fit model using training data\n",
        "print_progress(\"Starting Random Forest training with cross-validation...\")\n",
        "rf_model = cv_rf.fit(train_data)\n",
        "print_progress(\"Random Forest training completed.\")\n",
        "\n",
        "# Get best model and parameters\n",
        "best_rf_model = rf_model.bestModel\n",
        "print_progress(f\"Best maxDepth: {best_rf_model.getMaxDepth()}\")\n",
        "print_progress(f\"Best numTrees: {best_rf_model.getNumTrees}\")\n",
        "\n",
        "# Evaluate on training data\n",
        "train_predictions = best_rf_model.transform(train_data)\n",
        "train_accuracy = evaluator.evaluate(train_predictions)\n",
        "print_progress(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_predictions = best_rf_model.transform(test_data)\n",
        "test_accuracy = evaluator.evaluate(test_predictions)\n",
        "print_progress(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Cross-validation best accuracy\n",
        "best_index = rf_model.avgMetrics.index(max(rf_model.avgMetrics))\n",
        "best_cv_accuracy = rf_model.avgMetrics[best_index]\n",
        "print_progress(f\"Best Cross-Validation Accuracy: {best_cv_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5_M1UNXc6oL",
        "outputId": "e33f1b14-0000-450b-db73-1ae16be413d5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-13 04:42:10] Starting Random Forest training with cross-validation...\n",
            "[2025-04-13 05:25:38] Random Forest training completed.\n",
            "[2025-04-13 05:25:38] Best maxDepth: 10\n",
            "[2025-04-13 05:25:38] Best numTrees: 50\n",
            "[2025-04-13 05:26:53] Training Accuracy: 0.3023\n",
            "[2025-04-13 05:28:00] Test Accuracy: 0.3020\n",
            "[2025-04-13 05:28:00] Best Cross-Validation Accuracy: 0.3006\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}